{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scScope.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOdT6s2rDKXX",
        "colab_type": "text"
      },
      "source": [
        "# Про методы импутации\n",
        "\n",
        "Проблема такая: в исходных данных часто бывают NaNы, то есть часть данных может просто отсутствовать. Это сильно влияет на качество обучения сети. С этим можно бороться разными способами. В первую очередь, образцы, для которых часть данных отутствует, можно просто выкинуть, но тогда теряется значимая информация.\n",
        "\n",
        "Другие методы:\n",
        "1) игнорировать NaNы. Пока не понимаю, почему этот способ не универсальный и зачем нужно что-то придумывать и импутировать.\n",
        "2) некоторые алгоритмы умеют подбирать, что вставлять, based on the training loss reduction.\n",
        "\n",
        "Судя по всему, некоторые алгоритмы просто не умеют воспринимать NaNы, и приходится вставлять их до этапа training-a.<br/>\n",
        "3) вставлять среднее. Это просто, но не учитывает никаких закономерностей распределения значений. Плюс не работает с категориальными значениями (пол, наличие/отсутствие чего-то). Часто, кажется, может вносить bias.<br/>\n",
        "4) вставлять самое частое значение или константу. Это работает с категориальными значениями. Но это не учитывает закономерностей и вносит bias.<br/>\n",
        "5) k-NN (кун не нужны). Берет k ближайших соседей (подсчет feature similarity), от них берет weighted average. Это затратно, потому что в память приходится закачивать весь dataset. Еще пишут, что он чувствительный к outlierам. Пока не понимаю почему. Видимо, это играет роль не на стадии выбора соседей, а на стадии подсчета average. И тут мы знаем, как с ними бороться.<br/>\n",
        "6) joint modeling (JM) \n",
        "*#когда придумал подход, гуляя с собакой в парке, а потом наткнулся на статью о нем (где моя Нобелевка).*\n",
        "Это короче мы строим многомерное распределение по дате, а потом Марковские цепи, рас рас, и мы вставляем значения так, чтобы они под наше многомерное распределения. Это нормально работает, если дату можно нормально апроксимировать знакомым распределением. <br/>\n",
        "7) MICE (Imputation Using Multivariate Imputation by Chained Equation)\n",
        "Не очень пока понимаю, как оно работает, но суть в том, что он итеративный, то есть несколько раз подбирает каждую вставку. Судя по всему, он сначала вставляет значение, смотрит, какое там локальное распределение получается, в зависимости от него снова подбирает параметр. А может, я что-то не так поняла. Оно хорошо работает, если нет возможности построить адекватное многомерное распределение. <br/>\n",
        "8) Машин лернингом их подбирать. \n",
        "\n",
        "\n",
        "В нашем случае способ 1) не подходит, потому что авторы позиционируют scScope как штуку, позволяющую работать со слабо экспрессирующими генами. То есть NaNов там много, и если их игнорировать, будет не очень. По крайней мере от этого пропадет вся суть программы. Ее основная идея во множественной коррекции импутации. Пункт 2) - по той же причине не наш случай. \n",
        "Пробовать надо 5-7, наверное. Хотя вставлять итеративный MICE в итеративную архитектуру - это матрешка. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5iqX5Y3DNHt",
        "colab_type": "text"
      },
      "source": [
        "# Batch effect correction\n",
        "\n",
        "Sources of variability in experimentally derived data include measurement error in addition to the physical phenomena of interest. This measurement error is a combination of systematic components, originating from the measuring instrument and random measurement errors. Several novel biological technologies, such as mass cytometry and single-cell RNA-seq (scRNA-seq), are plagued with systematic errors that may severely affect statistical analysis if the data are not properly calibrated.\n",
        "\n",
        "\n",
        "Итак, методы борьбы с этим в случае с scRNA-seqом:\n",
        "\n",
        "\n",
        "1) SVA метод (Surrogate variable analysis)\n",
        "https://bioconductor.org/packages/devel/bioc/vignettes/sva/inst/doc/sva.pdf\n",
        "We model the data as a combination of known variables of interest, known adjustment variables (known batches) and unknown and unmeasured artifacts. A simple version of this model might relate gene expression for gene ion sample j(gij) to the phenotype for that sample yj, the known batch variable for that sample (aj)and an unknown artifact on that sample \n",
        "![](http://www.tran-med.com/article/2016/2411-2917-2-1-3/img_1.png)\n",
        ", where gij is the gene expression, bi0stands for the baseline expression, bi1yj is the phenotype effect for sample j (like age, gender…), ciaj stands for the known batch factor, diuj represent the unknown artifact, and the error term eij is the measurement error.\n",
        "\n",
        "2) Баесовский COMBAT метод. \n",
        "![](https://imgur.com/eYkrLXS.png)\n",
        "\n",
        "3) PCA метод\n",
        "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This transformation is defined such that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components. The principal components are the eigenvectors of the covariance matrix (which is also symmetric), so they are orthogonal.\n",
        "\n",
        "PCA method could be intuitively used in correcting for batch effects in microarray or RNA-seq data. First we use the principal component method to infer the data variance in different axes. As the greatest source of variance in gene expression studies is nearly always across batches rather than across biological groups [5], it would be straightforward to view the batch effect mostly in the first 2 PC axes. We have used PCA method to remove batch effect in RNA-seq data, and have showed the first 2 PC axes for each method before and after correcting for the batch effect, obtaining a direct impression of the performance of each method.\n",
        "\n",
        "4) Что я хочу попробовать - остаточные нейронные сети\n",
        "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5870543/\n",
        "\n",
        "5) Что делают они - используют обучаемую матрицу batch-коррекции\n",
        "![](https://imgur.com/AOVNDgR.png)\n",
        "Где B - та самая матрица. u - вектор-индикатор батч эффекта (One hot encoding, ненулевое значение обозначает бэтч xc). r - ReLU юнит.\n",
        "\n",
        "Информация о принадлежности к тому или иному батчу хранится в exp_batch_idx. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhq15hS2i4TE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exp_batch_id_shape = exp_batch_idx.get_shape().as_list()\n",
        "exp_batch_dim = exp_batch_id_shape[1]\n",
        "with tf.variable_scope('batch_effect_removal'):\n",
        "  batch_effect_para_weight = _variable_with_weight_decay('batch_effect_weight',\n",
        "                                                        [exp_batch_dim,\n",
        "                                                        input_dim],\n",
        "                                                        stddev=0, wd=0)\n",
        "\n",
        "  batch_effect_removal_layer = tf.matmul(\n",
        "      exp_batch_idx, batch_effect_para_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0GqaCqK3EC2",
        "colab_type": "text"
      },
      "source": [
        "#Аннотация кода"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5AYt1x29_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scScope is a deep-learning based approach designed to identify cell-type composition from large-scale scRNA-seq profiles.\n",
        "\n",
        "\"\"\"Добрый день, друзья. Сегодня мы познакомимся с потрясающим миром tensorflow (TF). TF построен на двух важнейших\n",
        "элементах: Tensor'ы и Flow. Подружимся с ними поближе:\n",
        "\n",
        "Tensor - n-размерная матрица, содержащая в себе элементы. В общем, тензор - математический объект, аналогичный векторам,\n",
        "но более обобщенный, представленный эрреем компонетнов, являющихся функциями координат пространства. Ранг тензора - количес\n",
        "тво измерений тензора (= размерность ведь?). Тензоры - главные элементы TF, они являются узлами вычислительного графа. Таким образом, граф\n",
        "представляет собой тензоры, линейно соединенные друг с другом взаимоотношением математических функций.\n",
        "\n",
        "Поток данных по тензорам - Flow.\n",
        "\n",
        "За подробностями - https://towardsdatascience.com/a-beginner-introduction-to-tensorflow-part-1-6d139e038278\n",
        "\n",
        "У TensorFlow существует 2 версии: TF CPU и TF GPU. Последний работает не на всех компьютерах, а только на ограниченном\n",
        "наборе видеокарт, поддерживающих технологию вычисления Nvidia CUDA. Нейронки на GPU работают и обучаются\n",
        "в десятки раз быстрее, чем на CPU. Поэтому всегда предпочтительней работать с ГПУшной версией.\n",
        "\n",
        "Разберем импорты:\"\"\"\n",
        "\n",
        "\n",
        "# __future__ содержит в себе служебные функции. Например, absolute_import меняет поведение import так, чтобы\n",
        "# правильно импортировались подмодули по их абсолютному пути (в данном случае .ops)\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "# Tensorflow CPU и TF GPU импортируются идентично, в зависимости от того, какая версия установлена в энвайроменте\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "# ops содержит вспомогательные функции, которые используются для освобождения памяти CPU и GPU, а также параллелизации\n",
        "# процессов\n",
        "from .ops import average_gradients, _variable_with_weight_decay, _variable_on_cpu\n",
        "# PhenoGraph is a clustering method designed for high-dimensional single-cell data. It works by creating a graph\n",
        "# (\"network\") representing phenotypic similarities between cells and then identifying communities in this graph.\n",
        "import phenograph\n",
        "# Метод кластеризации по K-средним с помощью одного класса\n",
        "from sklearn.cluster import KMeans\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bo6160XDg4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_large(train_data_path,\n",
        "                file_num,\n",
        "                cell_size,\n",
        "                gene_size,\n",
        "                latent_code_dim,\n",
        "                exp_batch_idx=0,\n",
        "                use_mask=True,\n",
        "                batch_size=64,\n",
        "                max_epoch=100,\n",
        "                epoch_per_check=100,\n",
        "                T=2,\n",
        "                encoder_layers=[],\n",
        "                decoder_layers=[],\n",
        "                learning_rate=0.0001,\n",
        "                beta1=0.05,\n",
        "                num_gpus=1\n",
        "                ):\n",
        "    '''\n",
        "    scScope training:\n",
        "\t  This function is used to train the scScope model on gene expression data\n",
        "    Parameters:\n",
        "      train_data_path:      File path of multiple small gene expression files. Each file is a cell_size * gene_size matrix stored in *.npy format.\n",
        "                                                    Files are named in \"batch_0.npy\", \"batch_1.npy\", ...\n",
        "      file_num:             Number of gene expression files in \"train_data_path\".\n",
        "      cell_size:            Cell numbers in each expression file. All files should include the same number of cells.\n",
        "      gene_size:            Gene numbers in each expression file. All files should include the same number of genes.\n",
        "      exp_batch_idx:        Number of experimental batches in the sequencing. if exp_batch_idx = 0, no batch information need to provide.\n",
        "                                                    Otherwise, experimental batch labels are stored in \"exp_batch_label_0.npy\", \"exp_batch_label_1.npy\", ..., corresponding to each data batch file.\n",
        "                                                    In each file, experimental batch labels are stored in an n * batch_num matrix in one-hot format. Experimental batch labels and data batch files\n",
        "                                                    are in the same directory.\n",
        "      latent_code_dim:      The feature dimension outputted by scScope.\n",
        "      batch_size:           Number of cells used in each training iteration.\n",
        "      max_epoch:            Maximal epoch used in training.\n",
        "      epoch_per_check:      Step to display current loss.\n",
        "      T:                    Depth of recurrence used in deep learning framework.\n",
        "      use_mask:             Flag indicating whether to use only non-zero entries in calculating losses.\n",
        "      learning_rate:        Step length in gradient descending algorithm.\n",
        "      beta1:                The beta1 parameter in AdamOptimizer.\n",
        "      num_gpus:             Number of gpus used for training in parallel.\n",
        "    Output:\n",
        "      model: a dataframe of scScope outputs with keys:\n",
        "                    'latent_code_session':      tensorflow session used in training.\n",
        "                    'test_input':               tensorflow dataholder for test data.\n",
        "                    'test_exp_batch_idx':       tensorflow dataholder for experimental batch label.\n",
        "                    'imputated_output':         imputed gene expressions.\n",
        "                    'latent_code':              latent features by scScope.\n",
        "                    'removed_batch_effect':     correcting layer learning by scScope.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "    # Мы будем тренить модель на одной gpu, поэтому считайте num_gpus = 1\n",
        "    # Здесь задается размер батча - пачки данных, которую мы будем единовременно подавать модели для обучения. \n",
        "    # Если сразу подавать модели все данные (batch_size = dataset_size), она будет занимать чрезмерно много памяти, \n",
        "    # поэтому делим на батчи. Есть так называемый stochaistic gradient descent, где мы подаем на вход лишь 1 \n",
        "    # instance, это другой экстрим. В таком случае модель будет обучаться дольше и хуже. Нужно подобрать такой размер\n",
        "    # батча, чтобы не занимать слишком много оперативки, и нформации в 1 батче хватало, чтобы нейронка смогла выявить \n",
        "    # закономерности.\n",
        "    batch_size = int(batch_size * num_gpus)\n",
        "    # Learning rate - гиперпараметр, который определяет, как сильно мы будем изменять веса и баясы по отношению к loss. \n",
        "    # Чем больше learning rate, тем быстрее loss (отклонение от правильных лейблов) стремится к 0, однако быстро не всегда хорошо.\n",
        "    # Нейронка может овершутнуть и пропустить локальный минимум, и в итоге к нему не сойтись, просто потому что шаг будет слишком большой. \n",
        "    # Чтобы побороть это, мы используем не слишком большой learning rate, а также ADAM optimiser, в котором по мере обучения корректируется \n",
        "    # штраф, накладываемый на ноды таким образом, чтобы не перепрыгнуть минимум.\n",
        "    \"\"\"Почему нельзя сделать как при титровании? я читала, что вместо лосс используется квадрат лосса, чтобы не минимизировать \n",
        "    функцию, которая может быть и отрицательной, но с этим можно и по-другому бороться. А дальше задать большой learning rate, \n",
        "    задетектить момент, когда знак ошибки меняется, вернуться к предыдущим weights, уменьшить learning rate и т.д.\"\"\"\n",
        "    learning_rate = learning_rate * num_gpus\n",
        "\n",
        "    if exp_batch_idx == 0:\n",
        "        exp_batch_idx_input = np.zeros((cell_size, 1))\n",
        "        consider_exp_batch = False\n",
        "    else:\n",
        "        consider_exp_batch = True\n",
        "\n",
        "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
        "    # Тензоры в тензорфлоу реализуются плейсхолдерами. Placeholder - объект, которому присваевается значение или \n",
        "    # тензор по мере прохождения информации по вычислительному графу. Здесь мы задаем плейсхолдер для трейна, который \n",
        "    # имеет размер (batch_size, gene_size(количество генов)), а также задаем плейсхолдер для реальных лейблов\n",
        "        train_data = tf.placeholder(\n",
        "            tf.float32, [batch_size, gene_size])\n",
        "        exp_batch_idx = tf.placeholder(tf.float32,\n",
        "                                       [batch_size, exp_batch_idx])\n",
        "\n",
        "        # Create an optimizer that performs gradient descent.\n",
        "        opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1)\n",
        "      \n",
        "        # Calculate the gradients on models deployed on each GPU then summarized the gradients.\n",
        "        # Градиенты - векторы, элементами которых являются частные производные функции предсказания по каждому \n",
        "        # направлению (???количества генов??? это как). Удобно представлять, что наша модель пытается выучить функцию размерности \n",
        "        # gene_size. Градиент просто указывает, в какую сторону нужно менять веса, чтобы аппроксимация стала лучше\n",
        "        tower_grads = []\n",
        "        tower_grads2 = []\n",
        "\n",
        "        with tf.variable_scope(tf.get_variable_scope()):\n",
        "        # Здесь строится вычислительный граф, состоящий из всех слоев и операций\n",
        "            for i in range(num_gpus):\n",
        "\n",
        "                print('Building Computational Graph on GPU-' + str(i))\n",
        "\n",
        "                with tf.device('/gpu:%d' % (i + 1)):\n",
        "\n",
        "                    with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
        "\n",
        "                        itv = int(batch_size / num_gpus)\n",
        "\n",
        "                        if i == 0:\n",
        "\n",
        "                            re_use_flag = False\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            re_use_flag = True\n",
        "\n",
        "                        loss = tower_loss(scope,\n",
        "                                          train_data[(i) *\n",
        "                                                     itv:(i + 1) * itv, :],\n",
        "                                          use_mask,\n",
        "                                          latent_code_dim,\n",
        "                                          T,\n",
        "                                          encoder_layers,\n",
        "                                          decoder_layers,\n",
        "                                          exp_batch_idx[(\n",
        "                                              i) * itv:(i + 1) * itv, :],\n",
        "                                          re_use_flag)\n",
        "\n",
        "                        tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "                        t_vars = tf.trainable_variables()\n",
        "\n",
        "                        inference_para = [\n",
        "                            var for var in t_vars if 'inference' in var.name]\n",
        "                        grads = opt.compute_gradients(loss, inference_para)\n",
        "                        tower_grads.append(grads)\n",
        "\n",
        "                        if consider_exp_batch:\n",
        "                            exp_batch_effect_para = [\n",
        "                                var for var in t_vars if 'batch_effect_removal' in var.name]\n",
        "                            grads2 = opt.compute_gradients(\n",
        "                                loss, exp_batch_effect_para)\n",
        "                            tower_grads2.append(grads2)\n",
        "\n",
        "        # Summarize gradients from multiple GPUs.\n",
        "        grads = average_gradients(tower_grads)\n",
        "        apply_gradient_op = opt.apply_gradients(grads)\n",
        "        train_op = apply_gradient_op\n",
        "\n",
        "        if consider_exp_batch:\n",
        "            grads2 = average_gradients(tower_grads2)\n",
        "            apply_gradient_op2 = opt.apply_gradients(grads2)\n",
        "            train_op2 = apply_gradient_op2\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # Configuration of GPU.\n",
        "        # Вычислительный граф деплоится на GPU\n",
        "        config_ = tf.ConfigProto()\n",
        "\n",
        "        config_.gpu_options.allow_growth = True\n",
        "\n",
        "        config_.allow_soft_placement = True\n",
        "        # По заданному вычислительному графу запускается сессия \n",
        "        # Короткий урок, чтобы понять, что такое сессия: https://danijar.com/what-is-a-tensorflow-session/\n",
        "        sess = tf.Session(config=config_)\n",
        "\n",
        "        sess.run(init)\n",
        "\n",
        "        reconstruction_error = []\n",
        "        # Начинается счетчик времени обучения\n",
        "        start = time.time()\n",
        "        \n",
        "        # Начинается итерация. max_epoch - количество полных итераций по всей дате\n",
        "        for step in range(1, max_epoch + 1):\n",
        "\n",
        "            for file_count in range(file_num):\n",
        "                # Чтобы не засорять оперативку, нужные файлы подгружаются только внутри цикла, \n",
        "                # и вне его сразу дампятся\n",
        "                train_data_real_val = np.load(\n",
        "                    train_data_path + '/batch_' + str(file_count) + '.npy')\n",
        "                if exp_batch_idx > 0:\n",
        "                    exp_batch_idx_input = np.load(\n",
        "                        train_data_path + '/exp_batch_label_' + str(file_count) + '.npy')\n",
        "\n",
        "                total_data_size = np.shape(train_data_real_val)[0]\n",
        "                total_sample_list = list(range(total_data_size))\n",
        "\n",
        "                total_cnt = total_data_size / (batch_size)\n",
        "\n",
        "                for itr_cnt in range(int(total_cnt)):\n",
        "\n",
        "                    sel_pos = random.sample(total_sample_list, batch_size)\n",
        "\n",
        "                    cur_data = train_data_real_val[sel_pos, :]\n",
        "                    cur_exp_batch_idx = exp_batch_idx_input[sel_pos, :]\n",
        "                # Важнейший метод для Session - run. Он запускает нужный фрагмент вычислительного графа \n",
        "                # для вычислить каждый тензор в train_op. Данные, по которым запускается вычислительный граф\n",
        "                # содержатся в словаре feed_dict. В данном случае, это наша train data и реальные лейблы (для вычисления лосса)\n",
        "                    sess.run(train_op,\n",
        "                             feed_dict={train_data: cur_data,\n",
        "                                        exp_batch_idx: cur_exp_batch_idx})\n",
        "                # Здесь исправляется бэтч эффект\n",
        "                    if consider_exp_batch:\n",
        "                        sess.run(train_op2,\n",
        "                                 feed_dict={train_data: cur_data,\n",
        "                                            exp_batch_idx: cur_exp_batch_idx})\n",
        "\n",
        "                if step % epoch_per_check == 0 and step > 0:\n",
        "\n",
        "                    all_input = tf.placeholder(\n",
        "                        tf.float32, [np.shape(train_data_real_val)[0], np.shape(train_data_real_val)[1]])\n",
        "                    exp_batch_idx_all = tf.placeholder(\n",
        "                        tf.float32, [np.shape(exp_batch_idx_input)[0], np.shape(exp_batch_idx_input)[1]])\n",
        "\n",
        "                    layer_output, train_latent_code, _ = Inference(\n",
        "                        all_input, latent_code_dim, T, encoder_layers, decoder_layers, exp_batch_idx_all, re_use=True)\n",
        "\n",
        "                    train_code_val, layer_output_val = sess.run(\n",
        "                        [train_latent_code[-1], layer_output[-1]],\n",
        "                        feed_dict={all_input: train_data_real_val, exp_batch_idx_all: exp_batch_idx_input})\n",
        "\n",
        "                    mask = np.sign(train_data_real_val)\n",
        "                    recon_error = np.linalg.norm(np.multiply(mask, layer_output_val) - np.multiply(\n",
        "                        mask, train_data_real_val)) / np.linalg.norm(np.multiply(mask, train_data_real_val))\n",
        "                    reconstruction_error.append(recon_error)\n",
        "                    print(\"Finisheded epoch：\" + str(step))\n",
        "                    print('Current reconstruction error is: ' + str(recon_error))\n",
        "                    # Здесь на каком-то задаваемом интервале выдается информация об обучении, а именно номер законченной эпохи, а также\n",
        "                    # текущая ошибка реконструкции (отклонение от того, что подавалось на вход)\n",
        "\n",
        "                    if len(reconstruction_error) >= 2:\n",
        "                        if (abs(reconstruction_error[-1] - reconstruction_error[-2]) / reconstruction_error[-2] < 1e-3):\n",
        "                            break\n",
        "\n",
        "        model = {}\n",
        "        # После окончания обучения заранее строится модель для теста (она подается на выход этой функции)\n",
        "        test_data_holder = tf.placeholder(\n",
        "            tf.float32, [None, gene_size])\n",
        "        test_exp_batch_idx = tf.placeholder(\n",
        "            tf.float32, [None, exp_batch_idx])\n",
        "\n",
        "        test_layer_out, test_latent_code, removed_batch_effect = Inference(\n",
        "            test_data_holder, latent_code_dim, T, encoder_layers, decoder_layers, test_exp_batch_idx, re_use=True)\n",
        "\n",
        "        model['latent_code_session'] = sess\n",
        "        model['test_input'] = test_data_holder\n",
        "        model['test_exp_batch_idx'] = test_exp_batch_idx\n",
        "        model['imputated_output'] = test_layer_out\n",
        "        model['latent_code'] = test_latent_code\n",
        "        model['removed_batch_effect'] = removed_batch_effect\n",
        "\n",
        "        duration = time.time() - start\n",
        "        print('Finish training ' + str(len(train_data)) + ' samples after ' + str(\n",
        "            step) + ' epochs. The total training time is ' +\n",
        "            str(duration) + ' seconds.')\n",
        "\n",
        "        return model\n",
        "\n",
        "\"\"\"Это уже после обучения запускается тест\"\"\"\n",
        "def predict_large(train_data_path,\n",
        "                  file_num,\n",
        "                  model,\n",
        "                  exp_batch_idx=0):\n",
        "    '''\n",
        "    Output the latent feature and imputed sequence for large scale dataset after training the model.\n",
        "    Parameters:\n",
        "            train_data_path:    The same data path as in \"train_large()\".\n",
        "            file_num:           Number of data files in train_data_path.\n",
        "            exp_batch_idx:      Number of experimental batches in sequencing. If exp_batch_idx=0, the function is run without batch correction.\n",
        "            model:              The pre-trained model by \"train_large()\".\n",
        "    Output:\n",
        "            Latent features and imputed genes for each data file.\n",
        "            For data file \"batch_i.npy\",  corresponding latent features and imputed gene expressions are stored in\n",
        "            \"feature_i.npy\" and \"imputation_i.npy\" files respectively in the same directory.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "    for file_count in range(file_num):\n",
        "\n",
        "        train_data = np.load(\n",
        "            train_data_path + '/batch_' + str(file_count) + '.npy')\n",
        "        if exp_batch_idx > 0:\n",
        "            batch_effect = np.load(\n",
        "                train_data_path + '/exp_batch_label_' + str(file_count) + '.npy')\n",
        "        else:\n",
        "            batch_effect = []\n",
        "        latent_fea, output_val, predicted_batch_effect = predict(\n",
        "            train_data, model, batch_effect=batch_effect)\n",
        "        np.save(train_data_path + '/feature_' +\n",
        "                str(file_count) + '.npy', latent_fea)\n",
        "        np.save(train_data_path + '/imputation_' +\n",
        "                str(file_count) + '.npy', output_val)\n",
        "\n",
        "\n",
        "def predict(test_data, model, batch_effect=[]):\n",
        "    '''\n",
        "    Make predications using the learned scScope model.\n",
        "\t\n",
        "    Parameter:\n",
        "            test_data:      gene expression matrix need to make prediction.\n",
        "            model:          pre-trained scScope model.\n",
        "    Output:\n",
        "            latent_fea:             scScope features for inputted gene expressions.\n",
        "            output_val:             gene expressions with imputations.\n",
        "            predicted_batch_effect: batch effects inferenced by scScope, if experimental batches exist.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    sess = model['latent_code_session']\n",
        "    test_data_holder = model['test_input']\n",
        "    test_exp_batch_idx_holder = model['test_exp_batch_idx']\n",
        "    output = model['imputated_output']\n",
        "    latent_code = model['latent_code']\n",
        "    removed_batch_effect = model['removed_batch_effect']\n",
        "    if len(batch_effect) == 0:\n",
        "        batch_effect_idx = np.zeros((np.shape(test_data)[0], 1))\n",
        "    else:\n",
        "        batch_effect_idx = batch_effect\n",
        "\n",
        "    for i in range(len(latent_code)):\n",
        "\n",
        "        latent_code_val, output_val, predicted_batch_effect = sess.run(\n",
        "            [latent_code[i], output[i], removed_batch_effect], feed_dict={\n",
        "                test_data_holder: test_data, test_exp_batch_idx_holder: batch_effect_idx})\n",
        "        if i == 0:\n",
        "            latent_fea = latent_code_val\n",
        "            output_total = output_val\n",
        "        else:\n",
        "            latent_fea = np.concatenate([latent_fea, latent_code_val], 1)\n",
        "            output_total = output_total + output_val\n",
        "\n",
        "    output_val = output_total / len(latent_code)\n",
        "    return latent_fea, output_val, predicted_batch_effect\n",
        "\n",
        "'''Сама архитектура'''\n",
        "#Поскольку количество строк одной матрицы связано с количеством столбцов другой, чтобы их можно было\n",
        "#перемножать, нужно каждый раз - для каждого слоя - прописывать размерность выходной матрицы\n",
        "def Inference(input_d, latent_code_dim, T, encoder_layers, decoder_layer, exp_batch_idx=[], re_use=False):\n",
        "    '''\n",
        "    The deep neural network structure of scScope\n",
        "    Parameters:\n",
        "\t\t\tinput_d:            gene expression matrix of dim n * m; n = number of cells, m = number of genes.\n",
        "            latent_code_dim:    the dimension of features outputted by scScope.\n",
        "            T:                  number of recurrent structures used in deep learning framework.\n",
        "            encoder_layers:\n",
        "            decoder_layer:\n",
        "            exp_batch_idx:      if provided, experimental batch labels are stored in an n * batch_num matrix in one-hot format.\n",
        "            re_use:             if re-use variables in training.\n",
        "    Output:\n",
        "            output_list:        outputs of decoder (y_c in the paper) in T recurrent structures.\n",
        "            latent_code_list:   latent representations (h_c in the paper) in T recurrent structures.\n",
        "            batch_effect_removal_layer:  experimental batch effects inferred by scScope.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "#это мы определяем размерность\n",
        "    input_shape = input_d.get_shape().as_list()\n",
        "#это вроде индексацияя количества столбцов. Судя по всему, в столбцах лежат RNAseq-и. И мы дальше с помощью этого dim-a сможет\n",
        "#изменять наши секи (добавлять/убирать)\n",
        "    input_dim = input_shape[1]\n",
        "#variable_scope - это создание группы. Нужно для того, чтобы было удобнее визуализировать в Tensor Board. Внутри лежат другие группы. \n",
        "    with tf.variable_scope('scScope') as scope_all:\n",
        "\n",
        "        if re_use == True:\n",
        "            scope_all.reuse_variables()\n",
        "\n",
        "        latent_code_list = []\n",
        "        output_list = []\n",
        "        exp_batch_id_shape = exp_batch_idx.get_shape().as_list()\n",
        "        exp_batch_dim = exp_batch_id_shape[1]\n",
        "        with tf.variable_scope('batch_effect_removal'):\n",
        "            batch_effect_para_weight = _variable_with_weight_decay('batch_effect_weight',\n",
        "                                                                   [exp_batch_dim,\n",
        "                                                                    input_dim],\n",
        "                                                                   stddev=0, wd=0)\n",
        "\n",
        "\"\"\"Мы работаем с огромным количеством данных. Внутри них есть batch-и - это группы данных, полученных в одной лабе при одних и тех же условиях.\n",
        "Между batch-ами меняются конфаундеры, и они получаются сдвинутыми друг относительно друга. Если мы знаем, что данные должны быть гомогенными,\n",
        "а они у нас гетерогенные, надо их как-то сдвинуть, то есть привести к одному мат ожиданию и одному стандартному отклонению. Обычно это делается\n",
        "до того, как кормить сетку, то есть при подготовке данных. В случае Batch-нормализации мат ожидание равно 0, а дисперсия - 1. Это решает \n",
        "проблему \"сдвига\" данных при прохождении по внутренним слоям. Нам кажется странным, как работает у них этот этап.Они перемножает матрицы \n",
        "ожидаемого match_id и para_weight. para_weight - это тензор, содержащий веса. \"\"\"\n",
        "\n",
        "            batch_effect_removal_layer = tf.matmul(\n",
        "                exp_batch_idx, batch_effect_para_weight)\n",
        "\n",
        "        with tf.variable_scope('inference'):\n",
        "            for i in range(T):\n",
        "                #Первая итерация обучения:\n",
        "                if i == 0:\n",
        "                    encoder_layer_list_W = []\n",
        "                    encoder_layer_list_b = []\n",
        "                    if len(encoder_layers) > 0:\n",
        "                        for l in range(len(encoder_layers)):\n",
        "                            if l == 0:\n",
        "                                encoder_layer_list_W.append(_variable_with_weight_decay('encoder_layer' + str(l),\n",
        "                                                                                        [input_dim,\n",
        "                                                                                         encoder_layers[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                encoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('encoder_layer_bias' + str(l), [encoder_layers[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                            else:\n",
        "                                encoder_layer_list_W.append(_variable_with_weight_decay('encoder_layer' + str(l),\n",
        "                                                                                        [encoder_layers[l - 1],\n",
        "                                                                                         encoder_layers[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                encoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('encoder_layer_bias' + str(l), [encoder_layers[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                        latent_code_layer_input_dim = encoder_layers[-1]\n",
        "\n",
        "                    else:\n",
        "                        latent_code_layer_input_dim = input_dim\n",
        "\n",
        "                    W_fea = _variable_with_weight_decay('latent_layer_weights',\n",
        "                                                        [latent_code_layer_input_dim,\n",
        "                                                         latent_code_dim],\n",
        "                                                        stddev=0.1, wd=0)\n",
        "                    b_fea = _variable_on_cpu('latent_layer_bias', [latent_code_dim],\n",
        "                                             tf.constant_initializer(0))\n",
        "\n",
        "                    decoder_layer_list_W = []\n",
        "                    decoder_layer_list_b = []\n",
        "                    if len(decoder_layer) > 0:\n",
        "                        for l in range(len(decoder_layer)):\n",
        "                            if l == 0:\n",
        "                                decoder_layer_list_W.append(_variable_with_weight_decay('dencoder_layer' + str(l),\n",
        "                                                                                        [latent_code_dim,\n",
        "                                                                                         decoder_layer[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                decoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('decoder_layer_bias' + str(l), [decoder_layer[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                            else:\n",
        "                                decoder_layer_list_W.append(_variable_with_weight_decay('dencoder_layer' + str(l),\n",
        "                                                                                        [decoder_layer[l - 1],\n",
        "                                                                                         decoder_layer[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                decoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('decoder_layer_bias' + str(l), [decoder_layer[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                        decoder_last_layer_dim = decoder_layer[-1]\n",
        "\n",
        "                    else:\n",
        "                        decoder_last_layer_dim = latent_code_dim\n",
        "\n",
        "                    W_recon = _variable_with_weight_decay('reconstruction_layer_weights',\n",
        "                                                          [decoder_last_layer_dim,\n",
        "                                                           input_dim],\n",
        "                                                          stddev=0.1, wd=0)\n",
        "                    b_recon = _variable_on_cpu('reconstruction_layer_bias', [input_dim],\n",
        "                                               tf.constant_initializer(0))\n",
        "                    input_vec = tf.nn.relu(\n",
        "                        input_d - batch_effect_removal_layer)\n",
        "                #Все следующие после первой итерации обучения:\n",
        "                else:\n",
        "                    #Вторая итерация \n",
        "                    if i == 1:\n",
        "                        W_feedback_1 = _variable_with_weight_decay('impute_layer_weights',\n",
        "                                                                   [input_dim, 64],\n",
        "                                                                   stddev=0.1, wd=0)\n",
        "                        b_feedback_1 = _variable_on_cpu(\n",
        "                            'impute_layer_bias', [64], tf.constant_initializer(0))\n",
        "\n",
        "                        W_feedback_2 = _variable_with_weight_decay('impute_layer_weights2',\n",
        "                                                                   [64, input_dim],\n",
        "                                                                   stddev=0.1, wd=0)\n",
        "                        b_feedback_2 = _variable_on_cpu(\n",
        "                            'impute_layer_bias2', [input_dim], tf.constant_initializer(0))\n",
        "                    #Здесь происходит магия импутации. Берем матрицу весов слоя импутации,\n",
        "                    #умножаем на матрицу аутпута, прибавляем баес, считаем функцию активации.\n",
        "                    intermediate_layer = tf.nn.relu(\n",
        "                        tf.matmul(output, W_feedback_1) + b_feedback_1)\n",
        "                    #Формируем слой импутации. (1 - tf.sign(input_d) = 0, если в позиции gene \n",
        "                    #expression matrix (input_d) есть значения и = 1, если там nan. \n",
        "                    imputation_layer = tf.multiply(\n",
        "                        1 - tf.sign(input_d), (tf.matmul(intermediate_layer, W_feedback_2) + b_feedback_2))\n",
        "\n",
        "                    input_vec = tf.nn.relu(\n",
        "                        imputation_layer + input_d - batch_effect_removal_layer)\n",
        "\n",
        "                intermedate_encoder_layer_list = []\n",
        "                if len(encoder_layer_list_W) > 0:\n",
        "                    for i in range(len(encoder_layer_list_W)):\n",
        "                        if i == 0:\n",
        "                            intermedate_encoder_layer_list.append(tf.nn.relu(\n",
        "                                tf.matmul(input_vec, encoder_layer_list_W[i]) + encoder_layer_list_b[i]))\n",
        "                        else:\n",
        "                            intermedate_encoder_layer_list.append(tf.nn.relu(tf.matmul(\n",
        "                                intermedate_encoder_layer_list[-1], encoder_layer_list_W[i]) + encoder_layer_list_b[i]))\n",
        "\n",
        "                    intermedate_encoder_layer = intermedate_encoder_layer_list[-1]\n",
        "                else:\n",
        "                    intermedate_encoder_layer = input_vec\n",
        "\n",
        "                latent_code = tf.nn.relu(\n",
        "                    tf.matmul(intermedate_encoder_layer, W_fea) + b_fea)\n",
        "\n",
        "                inter_decoder_layer_list = []\n",
        "\n",
        "                if len(decoder_layer_list_W) > 0:\n",
        "                    for i in range(len(decoder_layer_list_W)):\n",
        "                        if i == 0:\n",
        "                            inter_decoder_layer_list.append(tf.nn.relu(\n",
        "                                tf.matmul(latent_code, decoder_layer_list_W[i]) + decoder_layer_list_b[i]))\n",
        "                        else:\n",
        "                            inter_decoder_layer_list.append(tf.nn.relu(tf.matmul(\n",
        "                                inter_decoder_layer_list[-1], decoder_layer_list_W[i]) + decoder_layer_list_b[i]))\n",
        "                    inter_decoder_layer = inter_decoder_layer_list[-1]\n",
        "                else:\n",
        "                    inter_decoder_layer = latent_code\n",
        "\n",
        "                output = tf.nn.relu(\n",
        "                    tf.matmul(inter_decoder_layer, W_recon) + b_recon)\n",
        "                latent_code_list.append(latent_code)\n",
        "                output_list.append(output)\n",
        "\n",
        "        return output_list, latent_code_list, batch_effect_removal_layer\n",
        "\n",
        "\n",
        "def tower_loss(scope, batch_data, use_mask, latent_code_dim, T, encoder_layers, decoder_layers, exp_batch_id,\n",
        "               re_use_flag):\n",
        "    '''\n",
        "    Overall losses of scScope on multiple GPUs.\n",
        "    Parameter:\n",
        "            scope:              tensorflow name scope\n",
        "            batch_data:         cell batch for calculating the loss\n",
        "            use_mask:           flag indicating only use non-zero genes to calculate losses.\n",
        "            latent_code_dim:    the dimension of features outputted by scScope.\n",
        "            T:                  number of recurrent structures used in deep learning framework.\n",
        "            encoder_layers:\n",
        "            decoder_layers:\n",
        "            exp_batch_id:\n",
        "            re_use_flag:        if re-use variables in training.\n",
        "    Output:\n",
        "            total_loss:         total loss of multiple GPUs.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    layer_out, latent_code, batch_effect_removal_layer = Inference(\n",
        "        batch_data, latent_code_dim, T, encoder_layers, decoder_layers, exp_batch_id, re_use=re_use_flag)\n",
        "\n",
        "    _ = Cal_Loss(layer_out, batch_data, use_mask, batch_effect_removal_layer)\n",
        "\n",
        "    losses = tf.get_collection('losses', scope)\n",
        "\n",
        "    total_loss = tf.add_n(losses, name='total_loss')\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def Cal_Loss(outpout_layer_list, input_data, use_mask, removed_exp_batch_effect):\n",
        "    '''\n",
        "    Loss function of scScope.\n",
        "    Parameter:\n",
        "            outpout_layer_list:     encoder output of T recurrent structures in scScope.\n",
        "            input_data:             original gene expression matrix inputted into scScope.\n",
        "            use_mask:               flag indicating only use non-zero genes to calculate losses.\n",
        "            removed_exp_batch_effect:\n",
        "    Output:\n",
        "            acc_loss:               loss function value.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    input_data_corrected = input_data - removed_exp_batch_effect\n",
        "\n",
        "    if use_mask:\n",
        "        val_mask = tf.sign(input_data_corrected)\n",
        "    else:\n",
        "        val_mask = tf.sign(input_data_corrected + 1)\n",
        "\n",
        "    for i in range(len(outpout_layer_list)):\n",
        "        layer_out = outpout_layer_list[i]\n",
        "        if i == 0:\n",
        "            reconstruct_loss = tf.reduce_mean(\n",
        "                tf.norm(tf.multiply(val_mask, (layer_out - input_data_corrected))))\n",
        "        else:\n",
        "            reconstruct_loss = reconstruct_loss + \\\n",
        "                tf.reduce_mean(\n",
        "                    tf.norm(tf.multiply(val_mask, (layer_out - input_data_corrected))))\n",
        "    acc_loss = reconstruct_loss\n",
        "    tf.add_to_collection('losses', acc_loss)\n",
        "    return acc_loss\n",
        "\n",
        "\n",
        "def scalable_cluster(latent_code,\n",
        "                     kmeans_num=500,\n",
        "                     cluster_num=400,\n",
        "                     display_step=50,\n",
        "                     phenograh_neighbor=30\n",
        "                     ):\n",
        "    '''\n",
        "    Scalable  cluster:\n",
        "    To perform graph clustering on large-scale data, we designed a scalable clustering strategy by combining k-means and PhenoGraph.\n",
        "    Briefly, we divide cells into M (kmeans_num) groups of equal size and perform K-means (cluster_num) clustering on each group independently. \n",
        "\tThe whole dataset is split to M×K clusters and we only input the cluster centroids into PhenoGraph for graph clustering. \n",
        "\tFinally, each cell is assigned to graph clusters according to the cluster labels of its nearest centroids.\n",
        "    Parameters:\n",
        "        latent_code:    n*m matrix; n = number of cells, m = dimension of feature representation.\n",
        "        kmeans_num:     number of independent K-means clusterings used. This is also the subset number.\n",
        "        cluster_num:    cluster number for each K-means clustering. This is also the \"n_clusters\" in KMeans function in sklearn package.\n",
        "        display_step:   displaying the process of K-means clustering.\n",
        "        phenograh_neighbor: \"k\" parameter in PhenoGraph package.\n",
        "\t\t\n",
        "    Output:\n",
        "            label:          Cluster labels for input cells.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    print('Scalable clustering:')\n",
        "    print('Use %d subsets of cells for initially clustering...' % kmeans_num)\n",
        "\n",
        "    stamp = np.floor(np.linspace(0, latent_code.shape[0], kmeans_num + 1))\n",
        "    stamp = stamp.astype(int)\n",
        "\n",
        "    cluster_ceter = np.zeros([kmeans_num * cluster_num, latent_code.shape[1]])\n",
        "    mapping_sample_kmeans = np.zeros(latent_code.shape[0])\n",
        "\n",
        "    for i in range(kmeans_num):\n",
        "\n",
        "        low_bound = stamp[i]\n",
        "        upp_bound = stamp[i + 1]\n",
        "        sample_range = np.arange(low_bound, upp_bound)\n",
        "        select_sample = latent_code[sample_range, :]\n",
        "\n",
        "        kmeans = KMeans(n_clusters=cluster_num,\n",
        "                        random_state=0).fit(select_sample)\n",
        "        label = kmeans.labels_\n",
        "\n",
        "        for j in range(cluster_num):\n",
        "            cluster_sample_idx = np.nonzero(label == j)[0]\n",
        "            cluster_sample = select_sample[cluster_sample_idx, :]\n",
        "            cluster_ceter[i * cluster_num + j,\n",
        "                          :] = np.mean(cluster_sample, axis=0)\n",
        "            mapping_sample_kmeans[sample_range[cluster_sample_idx]\n",
        "                                  ] = i * cluster_num + j\n",
        "\n",
        "        if i % display_step == 0:\n",
        "            print('\\tK-means clustering for %d subset.' % i)\n",
        "\n",
        "    print('Finish intially clustering by K-means.')\n",
        "    print('Start PhenoGraph clustering...\\n')\n",
        "\n",
        "    label_pheno, graph, Q = phenograph.cluster(\n",
        "        cluster_ceter, k=phenograh_neighbor, n_jobs=1)\n",
        "\n",
        "    label = np.zeros(latent_code.shape[0])\n",
        "    for i in range(label_pheno.max() + 1):\n",
        "        center_index = np.nonzero(label_pheno == i)[0]\n",
        "        for j in center_index:\n",
        "            sample_index = np.nonzero(mapping_sample_kmeans == j)[\n",
        "                0]  # samples belong to this center\n",
        "            label[sample_index] = i\n",
        "    print('Finish density down-sampling clustering.')\n",
        "\n",
        "    return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkDKlhVOi5Pt",
        "colab_type": "text"
      },
      "source": [
        "   \"\" exp_batch_idx_input:  (optional) n * batch_num matrix in one-hot format, if provided, experimental batch ids are used for batch correction.\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1zNipcXJMSQ",
        "colab_type": "text"
      },
      "source": [
        "# Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3vAQMYdHbQm",
        "colab_type": "code",
        "outputId": "48e268c9-fc77-42a8-ff99-f082f36ee270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1040
        }
      },
      "source": [
        "!pip3 install git+https://github.com/jacoblevine/phenograph.git\n",
        "!pip3 install scanpy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/jacoblevine/phenograph.git\n",
            "  Cloning https://github.com/jacoblevine/phenograph.git to /tmp/pip-req-build-iyvlj7wd\n",
            "  Running command git clone -q https://github.com/jacoblevine/phenograph.git /tmp/pip-req-build-iyvlj7wd\n",
            "Requirement already satisfied: setuptools>=18.0.1 in /usr/local/lib/python3.6/dist-packages (from PhenoGraph==1.5.2) (41.0.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from PhenoGraph==1.5.2) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from PhenoGraph==1.5.2) (1.2.1)\n",
            "Requirement already satisfied: scikit_learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from PhenoGraph==1.5.2) (0.20.3)\n",
            "Requirement already satisfied: psutil>4 in /usr/local/lib/python3.6/dist-packages (from PhenoGraph==1.5.2) (5.4.8)\n",
            "Building wheels for collected packages: PhenoGraph\n",
            "  Building wheel for PhenoGraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t8bq6p5i/wheels/78/4e/51/59f41302009330e31c4a2d0dc1cbbdad17f4379471d40ab9af\n",
            "Successfully built PhenoGraph\n",
            "Installing collected packages: PhenoGraph\n",
            "Successfully installed PhenoGraph-1.5.2\n",
            "Collecting scanpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/93/438faab2024b56795ba8ac57f96fe20eb7647e91641af997ade381d19b16/scanpy-1.4.2.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 2.9MB/s \n",
            "\u001b[?25hCollecting anndata>=0.6.15 (from scanpy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/76/e641903db28455d73c148340967310ff50243c868f212da6a2bee925c655/anndata-0.6.19.tar.gz (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from scanpy) (3.0.3)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.6/dist-packages (from scanpy) (0.24.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scanpy) (1.2.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from scanpy) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from scanpy) (2.8.0)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from scanpy) (3.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from scanpy) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scanpy) (0.20.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from scanpy) (0.9.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from scanpy) (0.5.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from scanpy) (2.3)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.6/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from scanpy) (0.12.5)\n",
            "Collecting numba>=0.41.0 (from scanpy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/c6/b28f5cada2aca31018a8bb210065f9b6a3174d25a80a9961c7bd3e831c3e/numba-0.43.1-cp36-cp36m-manylinux1_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 21.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: umap-learn in /usr/local/lib/python3.6/dist-packages (from scanpy) (0.3.8)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from anndata>=0.6.15->scanpy) (1.16.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->scanpy) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->scanpy) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->scanpy) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->scanpy) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->scanpy) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->scanpy) (1.12.0)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->scanpy) (2.6.9)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->scanpy) (4.4.0)\n",
            "Requirement already satisfied: llvmlite>=0.28.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.41.0->scanpy) (0.28.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->scanpy) (41.0.1)\n",
            "Building wheels for collected packages: scanpy, anndata\n",
            "  Building wheel for scanpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/17/02/39181727a4496366cc662141a0a29d497bbdb0d9149981ddcb\n",
            "  Building wheel for anndata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/e5/74/a94b1e21a75fbdff29343aeb9970279fbc9ab8a2a8cfaa87dc\n",
            "Successfully built scanpy anndata\n",
            "Installing collected packages: anndata, numba, scanpy\n",
            "  Found existing installation: numba 0.40.1\n",
            "    Uninstalling numba-0.40.1:\n",
            "      Successfully uninstalled numba-0.40.1\n",
            "Successfully installed anndata-0.6.19 numba-0.43.1 scanpy-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnintBnzKL9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import phenograph\n",
        "\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3XewBeJCpEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ops.py\n",
        "\n",
        "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
        "    \"\"\"\n",
        "    Helper to create an initialized Variable with weight decay.\n",
        "    Note that the Variable is initialized with a truncated normal distribution.\n",
        "    A weight decay is added only if one is specified.\n",
        "    Args:\n",
        "      name: name of the variable\n",
        "      shape: list of ints\n",
        "      stddev: standard deviation of a truncated Gaussian\n",
        "      wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "          decay is not added for this Variable.\n",
        "    Returns:\n",
        "      Variable Tensor\n",
        "    Altschuler & Wu Lab 2018. \n",
        "    Software provided as is under Apache License 2.0.\n",
        "    \"\"\"\n",
        "\n",
        "    dtype = tf.float32\n",
        "\n",
        "    var = _variable_on_cpu(\n",
        "\n",
        "        name,\n",
        "\n",
        "        shape,\n",
        "\n",
        "        tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
        "\n",
        "    if wd != 0:\n",
        "\n",
        "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "\n",
        "        tf.add_to_collection('losses', weight_decay)\n",
        "\n",
        "    return var\n",
        "\n",
        "\n",
        "def _variable_on_cpu(name, shape, initializer):\n",
        "    \"\"\"Helper to create a Variable stored on CPU memory.\n",
        "    Args:\n",
        "      name: name of the variable\n",
        "      shape: list of ints\n",
        "      initializer: initializer for Variable\n",
        "    Returns:\n",
        "      Variable Tensor\n",
        "    Altschuler & Wu Lab 2018. \n",
        "    Software provided as is under Apache License 2.0.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "\n",
        "        dtype = tf.float32\n",
        "\n",
        "        var = tf.get_variable(\n",
        "            name, shape, initializer=initializer, dtype=dtype)\n",
        "\n",
        "    return var\n",
        "\n",
        "\n",
        "def average_gradients(tower_grads):\n",
        "    \"\"\" Summarize the gradient calculated by each GPU.\n",
        "    Altschuler & Wu Lab 2018. \n",
        "    Software provided as is under Apache License 2.0.\n",
        "    \"\"\"\n",
        "\n",
        "    average_grads = []\n",
        "\n",
        "    for grad_and_vars in zip(*tower_grads):\n",
        "\n",
        "        # Note that each grad_and_vars looks like the following:\n",
        "\n",
        "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
        "\n",
        "        grads = []\n",
        "\n",
        "        for g, var1 in grad_and_vars:\n",
        "\n",
        "            # Add 0 dimension to the gradients to represent the tower.\n",
        "\n",
        "            expanded_g = tf.expand_dims(g, 0)\n",
        "\n",
        "            # Append on a 'tower' dimension which we will average over below.\n",
        "\n",
        "            grads.append(expanded_g)\n",
        "\n",
        "        # Average over the 'tower' dimension.\n",
        "\n",
        "        grad = tf.concat(axis=0, values=grads)\n",
        "\n",
        "        grad = tf.reduce_mean(grad, 0)\n",
        "\n",
        "        v = grad_and_vars[0][1]\n",
        "\n",
        "        grad_and_var = (grad, v)\n",
        "\n",
        "        average_grads.append(grad_and_var)\n",
        "\n",
        "    return average_grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb7vgQTjJ9pM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# __init__.py\n",
        "\n",
        "# scScope is a deep-learning based approach designed to identify cell-type composition from large-scale scRNA-seq profiles.\n",
        "\n",
        "\n",
        "def train(train_data_set,\n",
        "          latent_code_dim,\n",
        "          use_mask=True,\n",
        "          batch_size=64,\n",
        "          max_epoch=100,\n",
        "          epoch_per_check=100,\n",
        "          T=2,\n",
        "          exp_batch_idx_input=[],\n",
        "          encoder_layers=[],\n",
        "          decoder_layers=[],\n",
        "          learning_rate=0.0001,\n",
        "          beta1=0.05,\n",
        "          num_gpus=1\n",
        "          ):\n",
        "    '''\n",
        "    scScope training:\n",
        "\t  This function is used to train the scScope model on gene expression data\n",
        "    Parameters:\n",
        "      train_data_set:       gene expression matrix of dim n * m; n = number of cells, m = number of genes.\n",
        "      latent_code_dim:      feature dimension outputted by scScope.\n",
        "      batch_size:           number of cells used in each training iteration.\n",
        "      max_epoch:            maximal epoch used in training.\n",
        "      epoch_per_check:      step to display current loss.\n",
        "      T:                    depth of recurrence used in deep learning framework.\n",
        "      use_mask:             flag indicating whether to use only non-zero entries in calculating losses.\n",
        "      learning_rate:        step length in gradient descending algorithm.\n",
        "      beta1:                beta1 parameter in AdamOptimizer.\n",
        "      num_gpus:             number of gpus used for training in parallel.\n",
        "      exp_batch_idx_input:  (optional) n * batch_num matrix in one-hot format, if provided, experimental batch ids are used for batch correction.\n",
        "      encoder_layers:       network structure for encoder layers of the autoencoder; e.g. [64,128] means adding two layers with 64 and 128 nodes between the input and hidden features\n",
        "      decoder_layers:       network structure for decoder layers of the autoencoder; e.g. [64,128] means adding two layers with 64 and 128 nodes between the hidden feature and the output layer\n",
        "    Output:\n",
        "      model: a dataframe of scScope outputs with keys:\n",
        "            'latent_code_session':      tensorflow session used in training.\n",
        "            'test_input':               tensorflow dataholder for test data.\n",
        "            'test_exp_batch_idx':       tensorflow dataholder for experimental batch label.\n",
        "            'imputated_output':         imputed gene expressions.\n",
        "            'latent_code':              latent features by scScope.\n",
        "            'removed_batch_effect':     correcting layer learning by scScope.\n",
        "    Altschuler & Wu Lab 2018. \n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    batch_size = int(batch_size*num_gpus)\n",
        "    learning_rate = learning_rate*num_gpus\n",
        "\n",
        "    if len(exp_batch_idx_input) == 0:\n",
        "        exp_batch_idx_input = np.zeros((np.shape(train_data_set)[0], 1))\n",
        "        consider_exp_batch = False\n",
        "    else:\n",
        "        consider_exp_batch = True\n",
        "\n",
        "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
        "\n",
        "        train_data = tf.placeholder(\n",
        "            tf.float32, [batch_size, np.shape(train_data_set)[1]])\n",
        "        exp_batch_idx = tf.placeholder(tf.float32,\n",
        "                                       [batch_size, np.shape(exp_batch_idx_input)[1]])\n",
        "\n",
        "        # Create an optimizer that performs gradient descent.\n",
        "\n",
        "        opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1)\n",
        "\n",
        "        # Calculate the gradients on models deployed on each GPU then summarize the gradients.\n",
        "        tower_grads = []\n",
        "        tower_grads2 = []\n",
        "\n",
        "        with tf.variable_scope(tf.get_variable_scope()):\n",
        "\n",
        "            for i in range(num_gpus):\n",
        "\n",
        "                print('Building Computational Graph on GPU-'+str(i))\n",
        "\n",
        "                with tf.device('/gpu:%d' % (i+1)):\n",
        "\n",
        "                    with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
        "\n",
        "                        itv = int(batch_size/num_gpus)\n",
        "\n",
        "                        if i == 0:\n",
        "\n",
        "                            re_use_flag = False\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            re_use_flag = True\n",
        "\n",
        "                        loss = tower_loss(scope,\n",
        "                                          train_data[(i) *\n",
        "                                                     itv:(i + 1) * itv, :],\n",
        "                                          use_mask,\n",
        "                                          latent_code_dim,\n",
        "                                          T,\n",
        "                                          encoder_layers,\n",
        "                                          decoder_layers,\n",
        "                                          exp_batch_idx[(\n",
        "                                              i) * itv:(i + 1) * itv, :],\n",
        "                                          re_use_flag)\n",
        "\n",
        "                        tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "                        t_vars = tf.trainable_variables()\n",
        "\n",
        "                        inference_para = [\n",
        "                            var for var in t_vars if 'inference' in var.name]\n",
        "                        grads = opt.compute_gradients(loss, inference_para)\n",
        "                        tower_grads.append(grads)\n",
        "\n",
        "                        if consider_exp_batch:\n",
        "                            exp_batch_effect_para = [\n",
        "                                var for var in t_vars if 'batch_effect_removal' in var.name]\n",
        "                            grads2 = opt.compute_gradients(\n",
        "                                loss, exp_batch_effect_para)\n",
        "                            tower_grads2.append(grads2)\n",
        "\n",
        "                        # Save gradients from different GPUs.\n",
        "\n",
        "        # Summarize gradients from multiple GPUs.\n",
        "        grads = average_gradients(tower_grads)\n",
        "        apply_gradient_op = opt.apply_gradients(grads)\n",
        "        train_op = apply_gradient_op\n",
        "\n",
        "        if consider_exp_batch:\n",
        "            grads2 = average_gradients(tower_grads2)\n",
        "            apply_gradient_op2 = opt.apply_gradients(grads2)\n",
        "            train_op2 = apply_gradient_op2\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # Configuration of GPUs.\n",
        "        config_ = tf.ConfigProto()\n",
        "\n",
        "        config_.gpu_options.allow_growth = True\n",
        "\n",
        "        config_.allow_soft_placement = True\n",
        "\n",
        "        sess = tf.Session(config=config_)\n",
        "\n",
        "        sess.run(init)\n",
        "\n",
        "        total_data_size = np.shape(train_data_set)[0]\n",
        "\n",
        "        total_sample_list = list(range(total_data_size))\n",
        "\n",
        "        reconstruction_error = []\n",
        "\n",
        "        start = time.time()\n",
        "        for step in range(1, max_epoch+1):\n",
        "\n",
        "            total_cnt = total_data_size/(batch_size)\n",
        "\n",
        "            for itr_cnt in range(int(total_cnt)):\n",
        "\n",
        "                sel_pos = random.sample(total_sample_list, batch_size)\n",
        "\n",
        "                cur_data = train_data_set[sel_pos, :]\n",
        "                cur_exp_batch_idx = exp_batch_idx_input[sel_pos, :]\n",
        "\n",
        "                sess.run(train_op,\n",
        "                         feed_dict={train_data: cur_data,\n",
        "                                    exp_batch_idx: cur_exp_batch_idx})\n",
        "                if consider_exp_batch:\n",
        "                    sess.run(train_op2,\n",
        "                             feed_dict={train_data: cur_data,\n",
        "                                        exp_batch_idx: cur_exp_batch_idx})\n",
        "\n",
        "            if step % epoch_per_check == 0 and step > 0:\n",
        "\n",
        "                all_input = tf.placeholder(\n",
        "                    tf.float32, [np.shape(train_data_set)[0], np.shape(train_data_set)[1]])\n",
        "                exp_batch_idx_all = tf.placeholder(\n",
        "                    tf.float32, [np.shape(exp_batch_idx_input)[0], np.shape(exp_batch_idx_input)[1]])\n",
        "\n",
        "                layer_output, train_latent_code, _ = Inference(\n",
        "                    all_input, latent_code_dim, T, encoder_layers, decoder_layers, exp_batch_idx_all, re_use=True)\n",
        "\n",
        "                train_code_val, layer_output_val = sess.run(\n",
        "                    [train_latent_code[-1], layer_output[-1]], feed_dict={all_input: train_data_set, exp_batch_idx_all: exp_batch_idx_input})\n",
        "\n",
        "                mask = np.sign(train_data_set)\n",
        "                recon_error = np.linalg.norm(np.multiply(mask, layer_output_val)-np.multiply(\n",
        "                    mask, train_data_set))/np.linalg.norm(np.multiply(mask, train_data_set))\n",
        "                reconstruction_error.append(recon_error)\n",
        "                print(\"Finisheded epoch：\" + str(step))\n",
        "                print('Current reconstruction error is: '+str(recon_error))\n",
        "\n",
        "                if len(reconstruction_error) >= 2:\n",
        "                    if (abs(reconstruction_error[-1] - reconstruction_error[-2])/reconstruction_error[-2] < 1e-3):\n",
        "                        break\n",
        "\n",
        "        Model = {}\n",
        "\n",
        "        test_data_holder = tf.placeholder(\n",
        "            tf.float32, [None, np.shape(train_data_set)[1]])\n",
        "        test_exp_batch_idx = tf.placeholder(\n",
        "            tf.float32, [None, np.shape(exp_batch_idx_input)[1]])\n",
        "\n",
        "        test_layer_out, test_latent_code, removed_batch_effect = Inference(\n",
        "            test_data_holder, latent_code_dim, T, encoder_layers, decoder_layers, test_exp_batch_idx, re_use=True)\n",
        "\n",
        "        Model['latent_code_session'] = sess\n",
        "        Model['test_input'] = test_data_holder\n",
        "        Model['test_exp_batch_idx'] = test_exp_batch_idx\n",
        "        Model['Imputated_output'] = test_layer_out\n",
        "        Model['latent_code'] = test_latent_code\n",
        "        Model['removed_batch_effect'] = removed_batch_effect\n",
        "\n",
        "        duration = time.time()-start\n",
        "        print('Finish training ' + str(len(train_data_set)) + ' samples after '+str(step)+' epochs. The total training time is ' +\n",
        "              str(duration)+' seconds.')\n",
        "\n",
        "        return Model\n",
        "\n",
        "\n",
        "def predict(test_data, model, batch_effect=[]):\n",
        "    '''\n",
        "    Make predications using the learned scScope model.\n",
        "    Parameter:\n",
        "        test_data:      input gene expression matrix.\n",
        "        model:          pre-trained scScope model.\n",
        "    Output:\n",
        "        latent_fea:             scScope features output.\n",
        "        output_val:             gene expressions with imputations.\n",
        "        predicted_batch_effect: batch effects inferenced by scScope, if experimental batches exist.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    sess = model['latent_code_session']\n",
        "    test_data_holder = model['test_input']\n",
        "    test_exp_batch_idx_holder = model['test_exp_batch_idx']\n",
        "    output = model['Imputated_output']\n",
        "    latent_code = model['latent_code']\n",
        "    removed_batch_effect = model['removed_batch_effect']\n",
        "    if len(batch_effect) == 0:\n",
        "        batch_effect_idx = np.zeros((np.shape(test_data)[0], 1))\n",
        "    else:\n",
        "        batch_effect_idx = batch_effect\n",
        "\n",
        "    for i in range(len(latent_code)):\n",
        "\n",
        "        latent_code_val, output_val, predicted_batch_effect = sess.run([latent_code[i], output[i], removed_batch_effect], feed_dict={\n",
        "            test_data_holder: test_data, test_exp_batch_idx_holder: batch_effect_idx})\n",
        "        if i == 0:\n",
        "            latent_fea = latent_code_val\n",
        "        else:\n",
        "            latent_fea = np.concatenate([latent_fea, latent_code_val], 1)\n",
        "\n",
        "    return latent_fea, output_val, predicted_batch_effect\n",
        "\n",
        "\n",
        "def Inference(input_d, latent_code_dim, T, encoder_layers, decoder_layer, exp_batch_idx=[], re_use=False):\n",
        "    '''\n",
        "    The deep neural network structure of scScope.\n",
        "    Parameters:\n",
        "        input_d:            gene expression matrix of dim n * m; n = number of cells, m = number of genes.\n",
        "        latent_code_dim:    the dimension of features outputted by scScope.\n",
        "        T:                  number of recurrent structures used in deep learning framework.\n",
        "        encoder_layers:     the network structure for encoder layers of the autoencoder.\n",
        "        decoder_layers:     the network structure for decoder layers of the autoencoder.\n",
        "        exp_batch_idx:      if provided, experimental batch labels are stored in an n * batch_num matrix in one-hot format.\n",
        "        re_use:             if re-use variables in training.\n",
        "    Output:\n",
        "        output_list:        outputs of decoder (y_c in the paper) in T recurrent structures.\n",
        "        latent_code_list:   latent representations (h_c in the paper) in T recurrent structures.\n",
        "        batch_effect_removal_layer:  experimental batch effects inferred by scScope.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    input_shape = input_d.get_shape().as_list()\n",
        "\n",
        "    input_dim = input_shape[1]\n",
        "\n",
        "    with tf.variable_scope('scScope') as scope_all:\n",
        "\n",
        "        if re_use == True:\n",
        "\n",
        "            scope_all.reuse_variables()\n",
        "\n",
        "        latent_code_list = []\n",
        "        output_list = []\n",
        "        exp_batch_id_shape = exp_batch_idx.get_shape().as_list()\n",
        "        exp_batch_dim = exp_batch_id_shape[1]\n",
        "        with tf.variable_scope('batch_effect_removal'):\n",
        "            batch_effect_para_weight = _variable_with_weight_decay('batch_effect_weight', [exp_batch_dim, input_dim],\n",
        "                                                                   stddev=0, wd=0)\n",
        "\n",
        "            batch_effect_removal_layer = tf.matmul(\n",
        "                exp_batch_idx, batch_effect_para_weight)\n",
        "\n",
        "        with tf.variable_scope('inference'):\n",
        "            for i in range(T):\n",
        "                if i == 0:\n",
        "                    encoder_layer_list_W = []\n",
        "                    encoder_layer_list_b = []\n",
        "                    if len(encoder_layers) > 0:\n",
        "                        for l in range(len(encoder_layers)):\n",
        "                            if l == 0:\n",
        "                                encoder_layer_list_W.append(_variable_with_weight_decay('encoder_layer' + str(l),\n",
        "                                                                                        [input_dim,\n",
        "                                                                                         encoder_layers[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                encoder_layer_list_b.append(_variable_on_cpu('encoder_layer_bias' + str(l), [encoder_layers[l]],\n",
        "                                                                             tf.constant_initializer(0)))\n",
        "                            else:\n",
        "                                encoder_layer_list_W.append(_variable_with_weight_decay('encoder_layer' + str(l),\n",
        "                                                                                        [encoder_layers[l-1],\n",
        "                                                                                         encoder_layers[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                encoder_layer_list_b.append(_variable_on_cpu('encoder_layer_bias' + str(l), [encoder_layers[l]],\n",
        "                                                                             tf.constant_initializer(0)))\n",
        "                        latent_code_layer_input_dim = encoder_layers[-1]\n",
        "\n",
        "                    else:\n",
        "                        latent_code_layer_input_dim = input_dim\n",
        "\n",
        "                    W_fea = _variable_with_weight_decay('latent_layer_weights',\n",
        "                                                        [latent_code_layer_input_dim,\n",
        "                                                         latent_code_dim],\n",
        "                                                        stddev=0.1, wd=0)\n",
        "                    b_fea = _variable_on_cpu('latent_layer_bias', [latent_code_dim],\n",
        "                                             tf.constant_initializer(0))\n",
        "\n",
        "                    decoder_layer_list_W = []\n",
        "                    decoder_layer_list_b = []\n",
        "                    if len(decoder_layer) > 0:\n",
        "                        for l in range(len(decoder_layer)):\n",
        "                            if l == 0:\n",
        "                                decoder_layer_list_W.append(_variable_with_weight_decay('dencoder_layer' + str(l),\n",
        "                                                                                        [latent_code_dim,\n",
        "                                                                                         decoder_layer[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                decoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('decoder_layer_bias' + str(l), [decoder_layer[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                            else:\n",
        "                                decoder_layer_list_W.append(_variable_with_weight_decay('dencoder_layer' + str(l),\n",
        "                                                                                        [decoder_layer[l - 1],\n",
        "                                                                                         decoder_layer[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                decoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('decoder_layer_bias' + str(l), [decoder_layer[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                        decoder_last_layer_dim = decoder_layer[-1]\n",
        "\n",
        "                    else:\n",
        "                        decoder_last_layer_dim = latent_code_dim\n",
        "\n",
        "                    W_recon = _variable_with_weight_decay('reconstruction_layer_weights',\n",
        "                                                          [decoder_last_layer_dim,\n",
        "                                                           input_dim],\n",
        "                                                          stddev=0.1, wd=0)\n",
        "                    b_recon = _variable_on_cpu('reconstruction_layer_bias', [input_dim],\n",
        "                                               tf.constant_initializer(0))\n",
        "                    input_vec = tf.nn.relu(input_d-batch_effect_removal_layer)\n",
        "                else:\n",
        "\n",
        "                    if i == 1:\n",
        "                        W_feedback_1 = _variable_with_weight_decay('impute_layer_weights',\n",
        "                                                                   [input_dim, 64],\n",
        "                                                                   stddev=0.1, wd=0)\n",
        "                        b_feedback_1 = _variable_on_cpu(\n",
        "                            'impute_layer_bias', [64], tf.constant_initializer(0))\n",
        "\n",
        "                        W_feedback_2 = _variable_with_weight_decay('impute_layer_weights2',\n",
        "                                                                   [64, input_dim],\n",
        "                                                                   stddev=0.1, wd=0)\n",
        "                        b_feedback_2 = _variable_on_cpu(\n",
        "                            'impute_layer_bias2', [input_dim], tf.constant_initializer(0))\n",
        "                        # else:\n",
        "                        #     W_feedback_2 = ops._variable_with_weight_decay('impute_layer_weights2',\n",
        "                        #                                                    [input_dim, input_dim],\n",
        "                        #                                                    stddev=0.1, wd=0)\n",
        "                        #     b_feedback_2 = ops._variable_on_cpu(\n",
        "                        #         'impute_layer_bias2', [input_dim], tf.constant_initializer(0))\n",
        "\n",
        "                    # if input_dim>500:\n",
        "                    intermediate_layer = tf.nn.relu(\n",
        "                        tf.matmul(output, W_feedback_1) + b_feedback_1)\n",
        "                    imputation_layer = tf.multiply(\n",
        "                        1-tf.sign(input_d),  (tf.matmul(intermediate_layer, W_feedback_2)+b_feedback_2))\n",
        "                    # else:\n",
        "                    #     imputation_layer = tf.multiply(1 - tf.sign(input_d),\n",
        "                    #                                    (tf.matmul(output, W_feedback_2) + b_feedback_2))\n",
        "\n",
        "                    input_vec = tf.nn.relu(\n",
        "                        imputation_layer+input_d-batch_effect_removal_layer)\n",
        "\n",
        "                intermedate_encoder_layer_list = []\n",
        "                if len(encoder_layer_list_W) > 0:\n",
        "                    for i in range(len(encoder_layer_list_W)):\n",
        "                        if i == 0:\n",
        "                            intermedate_encoder_layer_list.append(tf.nn.relu(\n",
        "                                tf.matmul(input_vec, encoder_layer_list_W[i])+encoder_layer_list_b[i]))\n",
        "                        else:\n",
        "                            intermedate_encoder_layer_list.append(tf.nn.relu(tf.matmul(\n",
        "                                intermedate_encoder_layer_list[-1], encoder_layer_list_W[i])+encoder_layer_list_b[i]))\n",
        "\n",
        "                    intermedate_encoder_layer = intermedate_encoder_layer_list[-1]\n",
        "                else:\n",
        "                    intermedate_encoder_layer = input_vec\n",
        "\n",
        "                latent_code = tf.nn.relu(\n",
        "                    tf.matmul(intermedate_encoder_layer, W_fea)+b_fea)\n",
        "\n",
        "                inter_decoder_layer_list = []\n",
        "\n",
        "                if len(decoder_layer_list_W) > 0:\n",
        "                    for i in range(len(decoder_layer_list_W)):\n",
        "                        if i == 0:\n",
        "                            inter_decoder_layer_list.append(tf.nn.relu(\n",
        "                                tf.matmul(latent_code, decoder_layer_list_W[i])+decoder_layer_list_b[i]))\n",
        "                        else:\n",
        "                            inter_decoder_layer_list.append(tf.nn.relu(tf.matmul(\n",
        "                                inter_decoder_layer_list[-1], decoder_layer_list_W[i])+decoder_layer_list_b[i]))\n",
        "                    inter_decoder_layer = inter_decoder_layer_list[-1]\n",
        "                else:\n",
        "                    inter_decoder_layer = latent_code\n",
        "\n",
        "                output = tf.nn.relu(\n",
        "                    tf.matmul(inter_decoder_layer, W_recon)+b_recon)\n",
        "                latent_code_list.append(latent_code)\n",
        "                output_list.append(output)\n",
        "\n",
        "        return output_list, latent_code_list, batch_effect_removal_layer\n",
        "\n",
        "\n",
        "def tower_loss(scope, batch_data, use_mask, latent_code_dim, T, encoder_layers, decoder_layers, exp_batch_id,\n",
        "               re_use_flag):\n",
        "    '''\n",
        "        Overall losses of scScope on multiple GPUs.\n",
        "        Parameter:\n",
        "                scope:              tensorflow name scope\n",
        "                batch_data:         cell batch for calculating the loss\n",
        "                use_mask:           flag indicating only use non-zero genes to calculate losses.\n",
        "                latent_code_dim:    the dimension of features outputted by scScope.\n",
        "                T:                  number of recurrent structures used in deep learning framework.\n",
        "                encoder_layers:     the network structure for encoder layers of the autoencoder.\n",
        "                decoder_layers:     the network structure for decoder layers of the autoencoder.\n",
        "                exp_batch_idx:      if provided, experimental batch labels are stored in an n * batch_num matrix in one-hot format.\n",
        "                re_use_flag:        if re-use variables in training.\n",
        "        Output:\n",
        "                total_loss:         total loss of multiple GPUs.\n",
        "        Altschuler & Wu Lab 2018.\n",
        "        Software provided as is under Apache License 2.0.\n",
        "        '''\n",
        "\n",
        "    layer_out, latent_code, batch_effect_removal_layer = Inference(\n",
        "        batch_data, latent_code_dim, T, encoder_layers, decoder_layers, exp_batch_id, re_use=re_use_flag)\n",
        "\n",
        "    _ = Cal_Loss(layer_out, batch_data, use_mask, batch_effect_removal_layer)\n",
        "\n",
        "    losses = tf.get_collection('losses', scope)\n",
        "\n",
        "    total_loss = tf.add_n(losses, name='total_loss')\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def Cal_Loss(outpout_layer_list, input_data, use_mask, removed_exp_batch_effect):\n",
        "    '''\n",
        "        Loss function of scScope.\n",
        "        Parameter:\n",
        "                outpout_layer_list:     encoder output of T recurrent structures in scScope.\n",
        "                input_data:             original gene expression matrix inputted into scScope.\n",
        "                use_mask:               flag indicating only use non-zero genes to calculate losses.\n",
        "                removed_exp_batch_effect: removed exeperimental batch effects\n",
        "        Output:\n",
        "                acc_loss:               loss function value.\n",
        "        Altschuler & Wu Lab 2018.\n",
        "        Software provided as is under Apache License 2.0.\n",
        "        '''\n",
        "\n",
        "    input_data_corrected = input_data - removed_exp_batch_effect\n",
        "\n",
        "    if use_mask:\n",
        "        val_mask = tf.sign(input_data_corrected)\n",
        "    else:\n",
        "        val_mask = tf.sign(input_data_corrected + 1)\n",
        "\n",
        "    for i in range(len(outpout_layer_list)):\n",
        "        layer_out = outpout_layer_list[i]\n",
        "        if i == 0:\n",
        "            reconstruct_loss = tf.reduce_mean(\n",
        "                tf.norm(tf.multiply(val_mask, (layer_out - input_data_corrected))))\n",
        "        else:\n",
        "            reconstruct_loss = reconstruct_loss + \\\n",
        "                tf.reduce_mean(\n",
        "                    tf.norm(tf.multiply(val_mask, (layer_out - input_data_corrected))))\n",
        "    acc_loss = reconstruct_loss\n",
        "    tf.add_to_collection('losses', acc_loss)\n",
        "    return acc_loss\n",
        "\n",
        "\n",
        "def scalable_cluster(latent_code,\n",
        "                     kmeans_num=500,\n",
        "                     cluster_num=400,\n",
        "                     display_step=50,\n",
        "                     phenograh_neighbor=30\n",
        "                     ):\n",
        "    '''\n",
        "    Scalable  cluster:\n",
        "    To perform graph clustering on large-scale data, we designed a scalable clustering strategy by combining k-means and PhenoGraph.\n",
        "    Briefly, we divide cells into M (kmeans_num) groups of equal size and perform K-means (cluster_num) clustering on each group independently. \n",
        "\tThe whole dataset is split to M×K clusters and we only input the cluster centroids into PhenoGraph for graph clustering. \n",
        "\tFinally, each cell is assigned to graph clusters according to the cluster labels of its nearest centroids.\n",
        "    Parameters:\n",
        "        latent_code:    n*m matrix; n = number of cells, m = dimension of feature representation.\n",
        "        kmeans_num:     number of independent K-means clusterings used. This is also the subset number.\n",
        "        cluster_num:    cluster number for each K-means clustering. This is also the \"n_clusters\" in KMeans function in sklearn package.\n",
        "        display_step:   displaying the process of K-means clustering.\n",
        "        phenograh_neighbor: \"k\" parameter in PhenoGraph package.\n",
        "    Output:\n",
        "        Cluster labels for input cells.\n",
        "    Altschuler & Wu Lab 2018. \n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    print('Scalable clustering:')\n",
        "    print('Use %d subsets of cells for initially clustering...' % kmeans_num)\n",
        "\n",
        "    stamp = np.floor(np.linspace(0, latent_code.shape[0], kmeans_num + 1))\n",
        "    stamp = stamp.astype(int)\n",
        "\n",
        "    cluster_ceter = np.zeros([kmeans_num * cluster_num, latent_code.shape[1]])\n",
        "    mapping_sample_kmeans = np.zeros(latent_code.shape[0])\n",
        "\n",
        "    for i in range(kmeans_num):\n",
        "\n",
        "        low_bound = stamp[i]\n",
        "        upp_bound = stamp[i + 1]\n",
        "        sample_range = np.arange(low_bound, upp_bound)\n",
        "        select_sample = latent_code[sample_range, :]\n",
        "\n",
        "        kmeans = KMeans(n_clusters=cluster_num,\n",
        "                        random_state=0).fit(select_sample)\n",
        "        label = kmeans.labels_\n",
        "\n",
        "        for j in range(cluster_num):\n",
        "            cluster_sample_idx = np.nonzero(label == j)[0]\n",
        "            cluster_sample = select_sample[cluster_sample_idx, :]\n",
        "            cluster_ceter[i * cluster_num + j,\n",
        "                          :] = np.mean(cluster_sample, axis=0)\n",
        "            mapping_sample_kmeans[sample_range[cluster_sample_idx]\n",
        "                                  ] = i * cluster_num + j\n",
        "\n",
        "        if i % display_step == 0:\n",
        "            print('\\tK-means clustering for %d subset.' % i)\n",
        "\n",
        "    print('Finish intially clustering by K-means.')\n",
        "    print('Start PhenoGraph clustering...\\n')\n",
        "\n",
        "    label_pheno, graph, Q = phenograph.cluster(\n",
        "        cluster_ceter, k=phenograh_neighbor, n_jobs=1)\n",
        "\n",
        "    label = np.zeros(latent_code.shape[0])\n",
        "    for i in range(label_pheno.max() + 1):\n",
        "        center_index = np.nonzero(label_pheno == i)[0]\n",
        "        for j in center_index:\n",
        "            sample_index = np.nonzero(mapping_sample_kmeans == j)[\n",
        "                0]  # samples belong to this center\n",
        "            label[sample_index] = i\n",
        "    print('Finish density down-sampling clustering.')\n",
        "\n",
        "    return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZsUwDukLtSO",
        "colab_type": "text"
      },
      "source": [
        "## Starting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd7cpHXXRT3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import rpy2\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxxWmPRzQmID",
        "colab_type": "code",
        "outputId": "ff3cd559-0bf6-46dc-f5ad-1b0269cd20a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "%%R\n",
        "install.packages('splatter')\n",
        "library(splatter)\n",
        "\n",
        "simulate <- function(nGroups=1, nGenes=1000, batchCells=2000, dropout=3)\n",
        "{\n",
        "  if (nGroups > 1) method <- 'groups'\n",
        "  else             method <- 'single'\n",
        "  \n",
        "  group.prob <- rep(1, nGroups) / nGroups\n",
        "  \n",
        "  # new splatter requires dropout.type\n",
        "  if ('dropout.type' %in% slotNames(newSplatParams())) {\n",
        "    if (dropout)\n",
        "      dropout.type <- 'experiment'\n",
        "    else\n",
        "      dropout.type <- 'none'\n",
        "    \n",
        "    sim <- splatSimulate(group.prob=group.prob, nGenes=nGenes, batchCells=batchCells,\n",
        "                         dropout.type=dropout.type, method=method,\n",
        "                         seed=0, dropout.shape=-1, dropout.mid=dropout)\n",
        "    \n",
        "  } else {\n",
        "    sim <- splatSimulate(group.prob=group.prob, nGenes=nGenes, batchCells=batchCells,\n",
        "                         dropout.present=!dropout, method=method,\n",
        "                         seed=0, dropout.shape=-1, dropout.mid=dropout)        \n",
        "  }\n",
        "  \n",
        "  counts     <- as.data.frame(t(counts(sim)))\n",
        "  truecounts <- as.data.frame(t(assays(sim)$TrueCounts))\n",
        "  \n",
        "  dropout    <- assays(sim)$Dropout\n",
        "  mode(dropout) <- 'integer'\n",
        "  \n",
        "  cellinfo   <- as.data.frame(colData(sim))\n",
        "  geneinfo   <- as.data.frame(rowData(sim))\n",
        "  \n",
        "  list(counts=counts,\n",
        "       cellinfo=cellinfo,\n",
        "       geneinfo=geneinfo,\n",
        "       truecounts=truecounts)\n",
        "}\n",
        "\n",
        "cell_num<-2000\n",
        "drop_out<-4\n",
        "group<-5\n",
        "sim <- simulate(nGroups=group,batchCells=cell_num,dropout = drop_out)\n",
        "\n",
        "counts <- sim$counts\n",
        "geneinfo <- sim$geneinfo\n",
        "cellinfo <- sim$cellinfo\n",
        "truecounts <- sim$truecounts\n",
        "\n",
        "write.csv(counts, file =paste(\"./counts.csv\",sep=\"\") )\n",
        "write.csv(cellinfo, file =paste( \"./cellinfo.csv\",sep=\"\"))\n",
        "write.csv(geneinfo, file = paste(\"./geneinf.csv\",sep=\"\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Error in library(splatter) : there is no package called ‘splatter’\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: In addition: \n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Warning message:\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: package ‘splatter’ is not available (for R version 3.6.0) \n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6emrbDNLsQZ",
        "colab_type": "code",
        "outputId": "c2aff42f-1335-458e-d4d6-f05fa3247981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "#demo.py\n",
        "\n",
        "'''The goal of this demo is to show how to identify cell subpopulations based on latent\n",
        "representations of gene expression learned by scScope.'''\n",
        "import pandas as pd\n",
        "import phenograph\n",
        "import pickle\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# For this demo we normalize data using scanpy which is not a required package for scScope.\n",
        "# To install, use: pip install scanpy\n",
        "import scanpy.api as sc\n",
        "\n",
        "\n",
        "def RUN_MAIN():\n",
        "\n",
        "    # 1. Load gene expression matrix of simulated data\n",
        "    # gene expression with simulated dropouts\n",
        "    counts_drop = pd.read_csv('counts_1.csv', header=0, index_col=0)\n",
        "    # ground trouth subpopulation assignment\n",
        "    cellinfo = pd.read_csv('cellinfo_1.csv', header=0, index_col=0)\n",
        "\n",
        "    group = cellinfo.Group\n",
        "    label_ground_truth = []\n",
        "    for g in group:\n",
        "        g = int(g.split('Group')[1])\n",
        "        label_ground_truth.append(g)\n",
        "\n",
        "    # 2. Normalize gene expression based on scanpy (normalize each cell to have same library size)\n",
        "    # matrix of cells x genes\n",
        "    gene_expression = sc.AnnData(counts_drop.values)\n",
        "    # normalize each cell to have same count number\n",
        "    sc.pp.normalize_per_cell(gene_expression)\n",
        "    # update datastructure to use normalized data\n",
        "    gene_expression = gene_expression.X\n",
        "\n",
        "    latent_dim = 50\n",
        "\n",
        "    # 3. scScope learning\n",
        "    if gene_expression.shape[0] >= 100000:\n",
        "        DI_model = train(\n",
        "            gene_expression, latent_dim, T=2, batch_size=512, max_epoch=10, num_gpus=1, epoch_per_check=25)\n",
        "    else:\n",
        "        DI_model = train(\n",
        "            gene_expression, latent_dim, T=2, batch_size=64, max_epoch=300, num_gpus=1, epoch_per_check=25)\n",
        "\n",
        "    # 4. latent representations and imputed expressions\n",
        "    latent_code, imputed_val, _ = predict(\n",
        "        gene_expression, DI_model)\n",
        "\n",
        "    # 5. graph clustering\n",
        "    if latent_code.shape[0] <= 10000:\n",
        "        label, _, _ = phenograph.cluster(latent_code)\n",
        "    else:\n",
        "        label = scalable_cluster(latent_code)\n",
        "\n",
        "    # evaluate\n",
        "    ARI = adjusted_rand_score(label, label_ground_truth)\n",
        "    print(ARI)\n",
        "\n",
        "    X_embedded = TSNE(n_components=2).fit_transform(latent_code)\n",
        "\n",
        "    # visualization of the subpopulation using tSNE\n",
        "    plt.figure()\n",
        "    for i in range(5):\n",
        "        idx = np.nonzero(label == i)[0]\n",
        "        plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], alpha=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    RUN_MAIN()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b82753c3de22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m representations of gene expression learned by scScope.'''\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mphenograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madjusted_rand_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'phenograph'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PygvuGAzfPGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = test\n",
        "\n",
        "latent_code, imputed_val = prerdict(test_data, DI_model, batch_effect=[])\n",
        "label, _,  _ = phenograph.cluster(latent_code)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
