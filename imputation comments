Проблема такая: в исходных данных часто бывают NaNы, то есть часть данных может просто отсутствовать. Это сильно влияет на качество обучения сети.
С этим можно бороться разными способами. В первую очередь, образцы, для которых часть данных отутствует, можно просто выкинуть, но тогда теряется 
значимая информация.
Другие методы: 
1) игнорировать NaNы.
2) некоторые алгоритмы умеют подбирать, что вставлять, based on the training loss reduction.
Некоторые алгоритмы просто не умеют воспринимать NaNы, и приходится вставлять их до этапа training-a.
3) вставлять среднее. Это просто, но не учитывает никаких закономерностей распределения значений. Плюс не работает с категориальными значениями 
(пол, наличие/отсутствие чего-то). Часто, кажется, может вносить bias.
4) вставлять самое частое значение или константу. Это работает с категориальными значениями. Но это не учитывает закономерностей и вносит bias.
5) k-NN (кун не нужны). Берет k ближайших соседей (подсчет feature similarity), от них берет weighted average. Это затратно, потому что в память 
приходится закачивать весь dataset. Еще пишут, что он чувствительный к outlierам. Пока не понимаю почему. Видимо, это играет роль не на стадии 
выбора соседей, а на стадии подсчета average. И тут мы знаем, как с ними бороться.
6) joint modeling (JM) #когда придумал подход, гуляя с собакой в парке, а потом наткнулся на статью о нем (где моя Нобелевка). Это короче мы строим
многомерное распределение по дате, а потом Марковские цепи, рас рас, и мы вставляем значения так, чтобы они под наше многомерное распределения. Это
нормально работает, если дату можно нормально апроксимировать знакомым распределением. 
7) MICE (Imputation Using Multivariate Imputation by Chained Equation) Не очень пока понимаю, как оно работает, но суть в том, что он итеративный, 
то есть несколько раз подбирает каждую вставку. Судя по всему, он сначала вставляет значение, смотрит, какое там локальное распределение получается,
в зависимости от него снова подбирает параметр. Оно хорошо работает, если нет возможности построить адекватное многомерное распределение. 
8) Подбирать машин лернингом. 

Как эта проблема решается в ScScope:
Импутация происходит только если мы запускаем процесс с количеством 
итераций больше 1, причем расчет значений для импутации осуществляется на каждой итерации заново на основе аутпута предыдущей итерации.  В этом суть
ScScope - уменьшать шум и пересчитывать imput на данных с уменьшенным шумом.'''
                    #Здесь происходит магия импутации. Берем матрицу весов слоя импутации,
                    #умножаем на матрицу аутпута, прибавляем баес, считаем функцию активации.
                    intermediate_layer = tf.nn.relu(
                        tf.matmul(output, W_feedback_1) + b_feedback_1)
                    #Формируем слой импутации. (1 - tf.sign(input_d) = 0, если в позиции gene 
                    #expression matrix (input_d) есть значения, и = 1, если там nan. Умножаем
                    #получившуюся на предыдущем этапе функцию активации на веса (не та же матрица,
                    #что на предыдущем этапе, чтобы учесть размерности), прибавляем баес. Вставляем
                    #то, что получилось, если в ячейке матрицы был NaN. В ином случае вставляем ноль.
                    imputation_layer = tf.multiply(
                        1 - tf.sign(input_d), (tf.matmul(intermediate_layer, W_feedback_2) + b_feedback_2))
                    #Собственно прибавляем к исходной матрице получившийся imputation_layer. 
                    input_vec = tf.nn.relu(
                        imputation_layer + input_d - batch_effect_removal_layer)
 То есть для расчета импута перемножаются активации и веса всех нейронов в ряду и в столбце. Это не учитывает особенности распределения.
 Возможно, способ k ближайших соседей давал бы лучший результат. Причем затратную стадию поиска соседей можно было бы производить только
 на первой итерации, а затем просто пересчитывать значения импута.
