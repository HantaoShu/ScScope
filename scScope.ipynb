    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0GqaCqK3EC2",
        "colab_type": "text"
      },
      "source": [
        "#Аннотация кода из файла AltschulerWu-Lab/scScope/scscope/scscope/large_scale_processing.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5AYt1x29_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scScope is a deep-learning based approach designed to identify cell-type composition from large-scale scRNA-seq profiles.\n",
        "\n",
        "\"\"\"Добрый день, друзья. Сегодня мы познакомимся с потрясающим миром tensorflow (TF). TF построен на двух важнейших\n",
        "элементах: Tensor'ы и Flow. Подружимся с ними поближе:\n",
        "\n",
        "Tensor - n-размерная матрица, содержащая в себе элементы. В общем, тензор - математический объект, аналогичный векторам,\n",
        "но более обобщенный, представленный эрреем компонетнов, являющихся функциями координат пространства. Ранг тензора - количес\n",
        "тво измерений тензора (= размерность ведь?). Тензоры - главные элементы TF, они являются узлами вычислительного графа. Таким образом, граф\n",
        "представляет собой тензоры, линейно соединенные друг с другом взаимоотношением математических функций.\n",
        "\n",
        "Поток данных по тензорам - Flow.\n",
        "\n",
        "За подробностями - https://towardsdatascience.com/a-beginner-introduction-to-tensorflow-part-1-6d139e038278\n",
        "\n",
        "У TensorFlow существует 2 версии: TF CPU и TF GPU. Последний работает не на всех компьютерах, а только на ограниченном\n",
        "наборе видеокарт, поддерживающих технологию вычисления Nvidia CUDA. Нейронки на GPU работают и обучаются\n",
        "в десятки раз быстрее, чем на CPU. Поэтому всегда предпочтительней работать с ГПУшной версией.\n",
        "\n",
        "Разберем импорты:\"\"\"\n",
        "\n",
        "\n",
        "# __future__ содержит в себе служебные функции. Например, absolute_import меняет поведение import так, чтобы\n",
        "# правильно импортировались подмодули по их абсолютному пути (в данном случае .ops)\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "# Tensorflow CPU и TF GPU импортируются идентично, в зависимости от того, какая версия установлена в энвайроменте\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "# ops содержит вспомогательные функции, которые используются для освобождения памяти CPU и GPU, а также параллелизации\n",
        "# процессов\n",
        "from .ops import average_gradients, _variable_with_weight_decay, _variable_on_cpu\n",
        "# PhenoGraph is a clustering method designed for high-dimensional single-cell data. It works by creating a graph\n",
        "# (\"network\") representing phenotypic similarities between cells and then identifying communities in this graph.\n",
        "import phenograph\n",
        "# Метод кластеризации по K-средним с помощью одного класса\n",
        "from sklearn.cluster import KMeans\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bo6160XDg4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_large(train_data_path,\n",
        "                file_num,\n",
        "                cell_size,\n",
        "                gene_size,\n",
        "                latent_code_dim,\n",
        "                exp_batch_idx=0,\n",
        "                use_mask=True,\n",
        "                batch_size=64,\n",
        "                max_epoch=100,\n",
        "                epoch_per_check=100,\n",
        "                T=2,\n",
        "                encoder_layers=[],\n",
        "                decoder_layers=[],\n",
        "                learning_rate=0.0001,\n",
        "                beta1=0.05,\n",
        "                num_gpus=1\n",
        "                ):\n",
        "    '''\n",
        "    scScope training:\n",
        "\t  This function is used to train the scScope model on gene expression data\n",
        "    Parameters:\n",
        "      train_data_path:      File path of multiple small gene expression files. Each file is a cell_size * gene_size matrix stored in *.npy format.\n",
        "                                                    Files are named in \"batch_0.npy\", \"batch_1.npy\", ...\n",
        "      file_num:             Number of gene expression files in \"train_data_path\".\n",
        "      cell_size:            Cell numbers in each expression file. All files should include the same number of cells.\n",
        "      gene_size:            Gene numbers in each expression file. All files should include the same number of genes.\n",
        "      exp_batch_idx:        Number of experimental batches in the sequencing. if exp_batch_idx = 0, no batch information need to provide.\n",
        "                                                    Otherwise, experimental batch labels are stored in \"exp_batch_label_0.npy\", \"exp_batch_label_1.npy\", ..., corresponding to each data batch file.\n",
        "                                                    In each file, experimental batch labels are stored in an n * batch_num matrix in one-hot format. Experimental batch labels and data batch files\n",
        "                                                    are in the same directory.\n",
        "      latent_code_dim:      The feature dimension outputted by scScope.\n",
        "      batch_size:           Number of cells used in each training iteration.\n",
        "      max_epoch:            Maximal epoch used in training.\n",
        "      epoch_per_check:      Step to display current loss.\n",
        "      T:                    Depth of recurrence used in deep learning framework.\n",
        "      use_mask:             Flag indicating whether to use only non-zero entries in calculating losses.\n",
        "      learning_rate:        Step length in gradient descending algorithm.\n",
        "      beta1:                The beta1 parameter in AdamOptimizer.\n",
        "      num_gpus:             Number of gpus used for training in parallel.\n",
        "    Output:\n",
        "      model: a dataframe of scScope outputs with keys:\n",
        "                    'latent_code_session':      tensorflow session used in training.\n",
        "                    'test_input':               tensorflow dataholder for test data.\n",
        "                    'test_exp_batch_idx':       tensorflow dataholder for experimental batch label.\n",
        "                    'imputated_output':         imputed gene expressions.\n",
        "                    'latent_code':              latent features by scScope.\n",
        "                    'removed_batch_effect':     correcting layer learning by scScope.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "    # Мы будем тренить модель на одной gpu, поэтому считайте num_gpus = 1\n",
        "    # Здесь задается размер батча - пачки данных, которую мы будем единовременно подавать модели для обучения. \n",
        "    # Если сразу подавать модели все данные (batch_size = dataset_size), она будет занимать чрезмерно много памяти, \n",
        "    # поэтому делим на батчи. Есть так называемый stochaistic gradient descent, где мы подаем на вход лишь 1 \n",
        "    # instance, это другой экстрим. В таком случае модель будет обучаться дольше и хуже. Нужно подобрать такой размер\n",
        "    # батча, чтобы не занимать слишком много оперативки, и нформации в 1 батче хватало, чтобы нейронка смогла выявить \n",
        "    # закономерности.\n",
        "    batch_size = int(batch_size * num_gpus)\n",
        "    # Learning rate - гиперпараметр, который определяет, как сильно мы будем изменять веса и баясы по отношению к loss. \n",
        "    # Чем больше learning rate, тем быстрее loss (отклонение от правильных лейблов) стремится к 0, однако быстро не всегда хорошо.\n",
        "    # Нейронка может овершутнуть и пропустить локальный минимум, и в итоге к нему не сойтись, просто потому что шаг будет слишком большой. \n",
        "    # Чтобы побороть это, мы используем не слишком большой learning rate, а также ADAM optimiser, в котором по мере обучения корректируется \n",
        "    # штраф, накладываемый на ноды таким образом, чтобы не перепрыгнуть минимум.\n",
        "    \"\"\"Почему нельзя сделать как при титровании? я читала, что вместо лосс используется квадрат лосса, чтобы не минимизировать \n",
        "    функцию, которая может быть и отрицательной, но с этим можно и по-другому бороться. А дальше задать большой learning rate, \n",
        "    задетектить момент, когда знак ошибки меняется, вернуться к предыдущим weights, уменьшить learning rate и т.д.\"\"\"\n",
        "    learning_rate = learning_rate * num_gpus\n",
        "\n",
        "    if exp_batch_idx == 0:\n",
        "        exp_batch_idx_input = np.zeros((cell_size, 1))\n",
        "        consider_exp_batch = False\n",
        "    else:\n",
        "        consider_exp_batch = True\n",
        "\n",
        "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
        "    # Тензоры в тензорфлоу реализуются плейсхолдерами. Placeholder - объект, которому присваевается значение или \n",
        "    # тензор по мере прохождения информации по вычислительному графу. Здесь мы задаем плейсхолдер для трейна, который \n",
        "    # имеет размер (batch_size, gene_size(количество генов)), а также задаем плейсхолдер для реальных лейблов\n",
        "        train_data = tf.placeholder(\n",
        "            tf.float32, [batch_size, gene_size])\n",
        "        exp_batch_idx = tf.placeholder(tf.float32,\n",
        "                                       [batch_size, exp_batch_idx])\n",
        "\n",
        "        # Create an optimizer that performs gradient descent.\n",
        "        opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1)\n",
        "      \n",
        "        # Calculate the gradients on models deployed on each GPU then summarized the gradients.\n",
        "        # Градиенты - векторы, элементами которых являются частные производные функции предсказания по каждому \n",
        "        # направлению (???количества генов??? это как). Удобно представлять, что наша модель пытается выучить функцию размерности \n",
        "        # gene_size. Градиент просто указывает, в какую сторону нужно менять веса, чтобы аппроксимация стала лучше\n",
        "        tower_grads = []\n",
        "        tower_grads2 = []\n",
        "\n",
        "        with tf.variable_scope(tf.get_variable_scope()):\n",
        "        # Здесь строится вычислительный граф, состоящий из всех слоев и операций\n",
        "            for i in range(num_gpus):\n",
        "\n",
        "                print('Building Computational Graph on GPU-' + str(i))\n",
        "\n",
        "                with tf.device('/gpu:%d' % (i + 1)):\n",
        "\n",
        "                    with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
        "\n",
        "                        itv = int(batch_size / num_gpus)\n",
        "\n",
        "                        if i == 0:\n",
        "\n",
        "                            re_use_flag = False\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            re_use_flag = True\n",
        "\n",
        "                        loss = tower_loss(scope,\n",
        "                                          train_data[(i) *\n",
        "                                                     itv:(i + 1) * itv, :],\n",
        "                                          use_mask,\n",
        "                                          latent_code_dim,\n",
        "                                          T,\n",
        "                                          encoder_layers,\n",
        "                                          decoder_layers,\n",
        "                                          exp_batch_idx[(\n",
        "                                              i) * itv:(i + 1) * itv, :],\n",
        "                                          re_use_flag)\n",
        "\n",
        "                        tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "                        t_vars = tf.trainable_variables()\n",
        "\n",
        "                        inference_para = [\n",
        "                            var for var in t_vars if 'inference' in var.name]\n",
        "                        grads = opt.compute_gradients(loss, inference_para)\n",
        "                        tower_grads.append(grads)\n",
        "\n",
        "                        if consider_exp_batch:\n",
        "                            exp_batch_effect_para = [\n",
        "                                var for var in t_vars if 'batch_effect_removal' in var.name]\n",
        "                            grads2 = opt.compute_gradients(\n",
        "                                loss, exp_batch_effect_para)\n",
        "                            tower_grads2.append(grads2)\n",
        "\n",
        "        # Summarize gradients from multiple GPUs.\n",
        "        grads = average_gradients(tower_grads)\n",
        "        apply_gradient_op = opt.apply_gradients(grads)\n",
        "        train_op = apply_gradient_op\n",
        "\n",
        "        if consider_exp_batch:\n",
        "            grads2 = average_gradients(tower_grads2)\n",
        "            apply_gradient_op2 = opt.apply_gradients(grads2)\n",
        "            train_op2 = apply_gradient_op2\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # Configuration of GPU.\n",
        "        # Вычислительный граф деплоится на GPU\n",
        "        config_ = tf.ConfigProto()\n",
        "\n",
        "        config_.gpu_options.allow_growth = True\n",
        "\n",
        "        config_.allow_soft_placement = True\n",
        "        # По заданному вычислительному графу запускается сессия \n",
        "        # Короткий урок, чтобы понять, что такое сессия: https://danijar.com/what-is-a-tensorflow-session/\n",
        "        sess = tf.Session(config=config_)\n",
        "\n",
        "        sess.run(init)\n",
        "\n",
        "        reconstruction_error = []\n",
        "        # Начинается счетчик времени обучения\n",
        "        start = time.time()\n",
        "        \n",
        "        # Начинается итерация. max_epoch - количество полных итераций по всей дате\n",
        "        for step in range(1, max_epoch + 1):\n",
        "\n",
        "            for file_count in range(file_num):\n",
        "                # Чтобы не засорять оперативку, нужные файлы подгружаются только внутри цикла, \n",
        "                # и вне его сразу дампятся\n",
        "                train_data_real_val = np.load(\n",
        "                    train_data_path + '/batch_' + str(file_count) + '.npy')\n",
        "                if exp_batch_idx > 0:\n",
        "                    exp_batch_idx_input = np.load(\n",
        "                        train_data_path + '/exp_batch_label_' + str(file_count) + '.npy')\n",
        "\n",
        "                total_data_size = np.shape(train_data_real_val)[0]\n",
        "                total_sample_list = list(range(total_data_size))\n",
        "\n",
        "                total_cnt = total_data_size / (batch_size)\n",
        "\n",
        "                for itr_cnt in range(int(total_cnt)):\n",
        "\n",
        "                    sel_pos = random.sample(total_sample_list, batch_size)\n",
        "\n",
        "                    cur_data = train_data_real_val[sel_pos, :]\n",
        "                    cur_exp_batch_idx = exp_batch_idx_input[sel_pos, :]\n",
        "                # Важнейший метод для Session - run. Он запускает нужный фрагмент вычислительного графа \n",
        "                # для вычислить каждый тензор в train_op. Данные, по которым запускается вычислительный граф\n",
        "                # содержатся в словаре feed_dict. В данном случае, это наша train data и реальные лейблы (для вычисления лосса)\n",
        "                    sess.run(train_op,\n",
        "                             feed_dict={train_data: cur_data,\n",
        "                                        exp_batch_idx: cur_exp_batch_idx})\n",
        "                # Здесь исправляется бэтч эффект\n",
        "                    if consider_exp_batch:\n",
        "                        sess.run(train_op2,\n",
        "                                 feed_dict={train_data: cur_data,\n",
        "                                            exp_batch_idx: cur_exp_batch_idx})\n",
        "\n",
        "                if step % epoch_per_check == 0 and step > 0:\n",
        "\n",
        "                    all_input = tf.placeholder(\n",
        "                        tf.float32, [np.shape(train_data_real_val)[0], np.shape(train_data_real_val)[1]])\n",
        "                    exp_batch_idx_all = tf.placeholder(\n",
        "                        tf.float32, [np.shape(exp_batch_idx_input)[0], np.shape(exp_batch_idx_input)[1]])\n",
        "\n",
        "                    layer_output, train_latent_code, _ = Inference(\n",
        "                        all_input, latent_code_dim, T, encoder_layers, decoder_layers, exp_batch_idx_all, re_use=True)\n",
        "\n",
        "                    train_code_val, layer_output_val = sess.run(\n",
        "                        [train_latent_code[-1], layer_output[-1]],\n",
        "                        feed_dict={all_input: train_data_real_val, exp_batch_idx_all: exp_batch_idx_input})\n",
        "\n",
        "                    mask = np.sign(train_data_real_val)\n",
        "                    recon_error = np.linalg.norm(np.multiply(mask, layer_output_val) - np.multiply(\n",
        "                        mask, train_data_real_val)) / np.linalg.norm(np.multiply(mask, train_data_real_val))\n",
        "                    reconstruction_error.append(recon_error)\n",
        "                    print(\"Finisheded epoch：\" + str(step))\n",
        "                    print('Current reconstruction error is: ' + str(recon_error))\n",
        "                    # Здесь на каком-то задаваемом интервале выдается информация об обучении, а именно номер законченной эпохи, а также\n",
        "                    # текущая ошибка реконструкции (отклонение от того, что подавалось на вход)\n",
        "\n",
        "                    if len(reconstruction_error) >= 2:\n",
        "                        if (abs(reconstruction_error[-1] - reconstruction_error[-2]) / reconstruction_error[-2] < 1e-3):\n",
        "                            break\n",
        "\n",
        "        model = {}\n",
        "        # После окончания обучения заранее строится модель для теста (она подается на выход этой функции)\n",
        "        test_data_holder = tf.placeholder(\n",
        "            tf.float32, [None, gene_size])\n",
        "        test_exp_batch_idx = tf.placeholder(\n",
        "            tf.float32, [None, exp_batch_idx])\n",
        "\n",
        "        test_layer_out, test_latent_code, removed_batch_effect = Inference(\n",
        "            test_data_holder, latent_code_dim, T, encoder_layers, decoder_layers, test_exp_batch_idx, re_use=True)\n",
        "\n",
        "        model['latent_code_session'] = sess\n",
        "        model['test_input'] = test_data_holder\n",
        "        model['test_exp_batch_idx'] = test_exp_batch_idx\n",
        "        model['imputated_output'] = test_layer_out\n",
        "        model['latent_code'] = test_latent_code\n",
        "        model['removed_batch_effect'] = removed_batch_effect\n",
        "\n",
        "        duration = time.time() - start\n",
        "        print('Finish training ' + str(len(train_data)) + ' samples after ' + str(\n",
        "            step) + ' epochs. The total training time is ' +\n",
        "            str(duration) + ' seconds.')\n",
        "\n",
        "        return model\n",
        "\n",
        "\"\"\"Это уже после обучения запускается тест\"\"\"\n",
        "def predict_large(train_data_path,\n",
        "                  file_num,\n",
        "                  model,\n",
        "                  exp_batch_idx=0):\n",
        "    '''\n",
        "    Output the latent feature and imputed sequence for large scale dataset after training the model.\n",
        "    Parameters:\n",
        "            train_data_path:    The same data path as in \"train_large()\".\n",
        "            file_num:           Number of data files in train_data_path.\n",
        "            exp_batch_idx:      Number of experimental batches in sequencing. If exp_batch_idx=0, the function is run without batch correction.\n",
        "            model:              The pre-trained model by \"train_large()\".\n",
        "    Output:\n",
        "            Latent features and imputed genes for each data file.\n",
        "            For data file \"batch_i.npy\",  corresponding latent features and imputed gene expressions are stored in\n",
        "            \"feature_i.npy\" and \"imputation_i.npy\" files respectively in the same directory.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "    for file_count in range(file_num):\n",
        "\n",
        "        train_data = np.load(\n",
        "            train_data_path + '/batch_' + str(file_count) + '.npy')\n",
        "        if exp_batch_idx > 0:\n",
        "            batch_effect = np.load(\n",
        "                train_data_path + '/exp_batch_label_' + str(file_count) + '.npy')\n",
        "        else:\n",
        "            batch_effect = []\n",
        "        latent_fea, output_val, predicted_batch_effect = predict(\n",
        "            train_data, model, batch_effect=batch_effect)\n",
        "        np.save(train_data_path + '/feature_' +\n",
        "                str(file_count) + '.npy', latent_fea)\n",
        "        np.save(train_data_path + '/imputation_' +\n",
        "                str(file_count) + '.npy', output_val)\n",
        "\n",
        "\n",
        "def predict(test_data, model, batch_effect=[]):\n",
        "    '''\n",
        "    Make predications using the learned scScope model.\n",
        "\t\n",
        "    Parameter:\n",
        "            test_data:      gene expression matrix need to make prediction.\n",
        "            model:          pre-trained scScope model.\n",
        "    Output:\n",
        "            latent_fea:             scScope features for inputted gene expressions.\n",
        "            output_val:             gene expressions with imputations.\n",
        "            predicted_batch_effect: batch effects inferenced by scScope, if experimental batches exist.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    sess = model['latent_code_session']\n",
        "    test_data_holder = model['test_input']\n",
        "    test_exp_batch_idx_holder = model['test_exp_batch_idx']\n",
        "    output = model['imputated_output']\n",
        "    latent_code = model['latent_code']\n",
        "    removed_batch_effect = model['removed_batch_effect']\n",
        "    if len(batch_effect) == 0:\n",
        "        batch_effect_idx = np.zeros((np.shape(test_data)[0], 1))\n",
        "    else:\n",
        "        batch_effect_idx = batch_effect\n",
        "\n",
        "    for i in range(len(latent_code)):\n",
        "\n",
        "        latent_code_val, output_val, predicted_batch_effect = sess.run(\n",
        "            [latent_code[i], output[i], removed_batch_effect], feed_dict={\n",
        "                test_data_holder: test_data, test_exp_batch_idx_holder: batch_effect_idx})\n",
        "        if i == 0:\n",
        "            latent_fea = latent_code_val\n",
        "            output_total = output_val\n",
        "        else:\n",
        "            latent_fea = np.concatenate([latent_fea, latent_code_val], 1)\n",
        "            output_total = output_total + output_val\n",
        "\n",
        "    output_val = output_total / len(latent_code)\n",
        "    return latent_fea, output_val, predicted_batch_effect\n",
        "\n",
        "'''Сама архитектура'''\n",
        "#Поскольку количество строк одной матрицы связано с количеством столбцов другой, чтобы их можно было\n",
        "#перемножать, нужно каждый раз - для каждого слоя - прописывать размерность выходной матрицы\n",
        "def Inference(input_d, latent_code_dim, T, encoder_layers, decoder_layer, exp_batch_idx=[], re_use=False):\n",
        "    '''\n",
        "    The deep neural network structure of scScope\n",
        "    Parameters:\n",
        "\t\t\tinput_d:            gene expression matrix of dim n * m; n = number of cells, m = number of genes.\n",
        "            latent_code_dim:    the dimension of features outputted by scScope.\n",
        "            T:                  number of recurrent structures used in deep learning framework.\n",
        "            encoder_layers:\n",
        "            decoder_layer:\n",
        "            exp_batch_idx:      if provided, experimental batch labels are stored in an n * batch_num matrix in one-hot format.\n",
        "            re_use:             if re-use variables in training.\n",
        "    Output:\n",
        "            output_list:        outputs of decoder (y_c in the paper) in T recurrent structures.\n",
        "            latent_code_list:   latent representations (h_c in the paper) in T recurrent structures.\n",
        "            batch_effect_removal_layer:  experimental batch effects inferred by scScope.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "#это мы определяем размерность\n",
        "    input_shape = input_d.get_shape().as_list()\n",
        "#это вроде индексацияя количества столбцов. Судя по всему, в столбцах лежат RNAseq-и. И мы дальше с помощью этого dim-a сможет\n",
        "#изменять наши секи (добавлять/убирать)\n",
        "    input_dim = input_shape[1]\n",
        "#variable_scope - это создание группы. Нужно для того, чтобы было удобнее визуализировать в Tensor Board. Внутри лежат другие группы. \n",
        "    with tf.variable_scope('scScope') as scope_all:\n",
        "\n",
        "        if re_use == True:\n",
        "            scope_all.reuse_variables()\n",
        "\n",
        "        latent_code_list = []\n",
        "        output_list = []\n",
        "        exp_batch_id_shape = exp_batch_idx.get_shape().as_list()\n",
        "        exp_batch_dim = exp_batch_id_shape[1]\n",
        "        with tf.variable_scope('batch_effect_removal'):\n",
        "            batch_effect_para_weight = _variable_with_weight_decay('batch_effect_weight',\n",
        "                                                                   [exp_batch_dim,\n",
        "                                                                    input_dim],\n",
        "                                                                   stddev=0, wd=0)\n",
        "\n",
        "\"\"\"Мы работаем с огромным количеством данных. Внутри них есть batch-и - это группы данных, полученных в одной лабе при одних и тех же условиях.\n",
        "Между batch-ами меняются конфаундеры, и они получаются сдвинутыми друг относительно друга. Если мы знаем, что данные должны быть гомогенными,\n",
        "а они у нас гетерогенные, надо их как-то сдвинуть, то есть привести к одному мат ожиданию и одному стандартному отклонению. Обычно это делается\n",
        "до того, как кормить сетку, то есть при подготовке данных. В случае Batch-нормализации мат ожидание равно 0, а дисперсия - 1. Это решает \n",
        "проблему \"сдвига\" данных при прохождении по внутренним слоям. Нам кажется странным, как работает у них этот этап.Они перемножает матрицы \n",
        "ожидаемого match_id и para_weight. para_weight - это тензор, содержащий веса. \"\"\"\n",
        "\n",
        "            batch_effect_removal_layer = tf.matmul(\n",
        "                exp_batch_idx, batch_effect_para_weight)\n",
        "\n",
        "        with tf.variable_scope('inference'):\n",
        "            for i in range(T):\n",
        "                #Первая итерация обучения:\n",
        "                if i == 0:\n",
        "                    encoder_layer_list_W = []\n",
        "                    encoder_layer_list_b = []\n",
        "                    if len(encoder_layers) > 0:\n",
        "                        for l in range(len(encoder_layers)):\n",
        "                            if l == 0:\n",
        "                                encoder_layer_list_W.append(_variable_with_weight_decay('encoder_layer' + str(l),\n",
        "                                                                                        [input_dim,\n",
        "                                                                                         encoder_layers[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                encoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('encoder_layer_bias' + str(l), [encoder_layers[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                            else:\n",
        "                                encoder_layer_list_W.append(_variable_with_weight_decay('encoder_layer' + str(l),\n",
        "                                                                                        [encoder_layers[l - 1],\n",
        "                                                                                         encoder_layers[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                encoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('encoder_layer_bias' + str(l), [encoder_layers[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                        latent_code_layer_input_dim = encoder_layers[-1]\n",
        "\n",
        "                    else:\n",
        "                        latent_code_layer_input_dim = input_dim\n",
        "\n",
        "                    W_fea = _variable_with_weight_decay('latent_layer_weights',\n",
        "                                                        [latent_code_layer_input_dim,\n",
        "                                                         latent_code_dim],\n",
        "                                                        stddev=0.1, wd=0)\n",
        "                    b_fea = _variable_on_cpu('latent_layer_bias', [latent_code_dim],\n",
        "                                             tf.constant_initializer(0))\n",
        "\n",
        "                    decoder_layer_list_W = []\n",
        "                    decoder_layer_list_b = []\n",
        "                    if len(decoder_layer) > 0:\n",
        "                        for l in range(len(decoder_layer)):\n",
        "                            if l == 0:\n",
        "                                decoder_layer_list_W.append(_variable_with_weight_decay('dencoder_layer' + str(l),\n",
        "                                                                                        [latent_code_dim,\n",
        "                                                                                         decoder_layer[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                decoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('decoder_layer_bias' + str(l), [decoder_layer[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                            else:\n",
        "                                decoder_layer_list_W.append(_variable_with_weight_decay('dencoder_layer' + str(l),\n",
        "                                                                                        [decoder_layer[l - 1],\n",
        "                                                                                         decoder_layer[l]],\n",
        "                                                                                        stddev=0.1, wd=0))\n",
        "                                decoder_layer_list_b.append(\n",
        "                                    _variable_on_cpu('decoder_layer_bias' + str(l), [decoder_layer[l]],\n",
        "                                                     tf.constant_initializer(0)))\n",
        "                        decoder_last_layer_dim = decoder_layer[-1]\n",
        "\n",
        "                    else:\n",
        "                        decoder_last_layer_dim = latent_code_dim\n",
        "\n",
        "                    W_recon = _variable_with_weight_decay('reconstruction_layer_weights',\n",
        "                                                          [decoder_last_layer_dim,\n",
        "                                                           input_dim],\n",
        "                                                          stddev=0.1, wd=0)\n",
        "                    b_recon = _variable_on_cpu('reconstruction_layer_bias', [input_dim],\n",
        "                                               tf.constant_initializer(0))\n",
        "                    input_vec = tf.nn.relu(\n",
        "                        input_d - batch_effect_removal_layer)\n",
        "                #Все следующие после первой итерации обучения:\n",
        "                else:\n",
        "                    #Вторая итерация \n",
        "                    if i == 1:\n",
        "                        W_feedback_1 = _variable_with_weight_decay('impute_layer_weights',\n",
        "                                                                   [input_dim, 64],\n",
        "                                                                   stddev=0.1, wd=0)\n",
        "                        b_feedback_1 = _variable_on_cpu(\n",
        "                            'impute_layer_bias', [64], tf.constant_initializer(0))\n",
        "\n",
        "                        W_feedback_2 = _variable_with_weight_decay('impute_layer_weights2',\n",
        "                                                                   [64, input_dim],\n",
        "                                                                   stddev=0.1, wd=0)\n",
        "                        b_feedback_2 = _variable_on_cpu(\n",
        "                            'impute_layer_bias2', [input_dim], tf.constant_initializer(0))\n",
        "                    #Здесь происходит магия импутации. Берем матрицу весов слоя импутации,\n",
        "                    #умножаем на матрицу аутпута, прибавляем баес, считаем функцию активации.\n",
        "                    intermediate_layer = tf.nn.relu(\n",
        "                        tf.matmul(output, W_feedback_1) + b_feedback_1)\n",
        "                    #Формируем слой импутации. (1 - tf.sign(input_d) = 0, если в позиции gene \n",
        "                    #expression matrix (input_d) есть значения и = 1, если там nan. \n",
        "                    imputation_layer = tf.multiply(\n",
        "                        1 - tf.sign(input_d), (tf.matmul(intermediate_layer, W_feedback_2) + b_feedback_2))\n",
        "\n",
        "                    input_vec = tf.nn.relu(\n",
        "                        imputation_layer + input_d - batch_effect_removal_layer)\n",
        "\n",
        "                intermedate_encoder_layer_list = []\n",
        "                if len(encoder_layer_list_W) > 0:\n",
        "                    for i in range(len(encoder_layer_list_W)):\n",
        "                        if i == 0:\n",
        "                            intermedate_encoder_layer_list.append(tf.nn.relu(\n",
        "                                tf.matmul(input_vec, encoder_layer_list_W[i]) + encoder_layer_list_b[i]))\n",
        "                        else:\n",
        "                            intermedate_encoder_layer_list.append(tf.nn.relu(tf.matmul(\n",
        "                                intermedate_encoder_layer_list[-1], encoder_layer_list_W[i]) + encoder_layer_list_b[i]))\n",
        "\n",
        "                    intermedate_encoder_layer = intermedate_encoder_layer_list[-1]\n",
        "                else:\n",
        "                    intermedate_encoder_layer = input_vec\n",
        "\n",
        "                latent_code = tf.nn.relu(\n",
        "                    tf.matmul(intermedate_encoder_layer, W_fea) + b_fea)\n",
        "\n",
        "                inter_decoder_layer_list = []\n",
        "\n",
        "                if len(decoder_layer_list_W) > 0:\n",
        "                    for i in range(len(decoder_layer_list_W)):\n",
        "                        if i == 0:\n",
        "                            inter_decoder_layer_list.append(tf.nn.relu(\n",
        "                                tf.matmul(latent_code, decoder_layer_list_W[i]) + decoder_layer_list_b[i]))\n",
        "                        else:\n",
        "                            inter_decoder_layer_list.append(tf.nn.relu(tf.matmul(\n",
        "                                inter_decoder_layer_list[-1], decoder_layer_list_W[i]) + decoder_layer_list_b[i]))\n",
        "                    inter_decoder_layer = inter_decoder_layer_list[-1]\n",
        "                else:\n",
        "                    inter_decoder_layer = latent_code\n",
        "\n",
        "                output = tf.nn.relu(\n",
        "                    tf.matmul(inter_decoder_layer, W_recon) + b_recon)\n",
        "                latent_code_list.append(latent_code)\n",
        "                output_list.append(output)\n",
        "\n",
        "        return output_list, latent_code_list, batch_effect_removal_layer\n",
        "\n",
        "\n",
        "def tower_loss(scope, batch_data, use_mask, latent_code_dim, T, encoder_layers, decoder_layers, exp_batch_id,\n",
        "               re_use_flag):\n",
        "    '''\n",
        "    Overall losses of scScope on multiple GPUs.\n",
        "    Parameter:\n",
        "            scope:              tensorflow name scope\n",
        "            batch_data:         cell batch for calculating the loss\n",
        "            use_mask:           flag indicating only use non-zero genes to calculate losses.\n",
        "            latent_code_dim:    the dimension of features outputted by scScope.\n",
        "            T:                  number of recurrent structures used in deep learning framework.\n",
        "            encoder_layers:\n",
        "            decoder_layers:\n",
        "            exp_batch_id:\n",
        "            re_use_flag:        if re-use variables in training.\n",
        "    Output:\n",
        "            total_loss:         total loss of multiple GPUs.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    layer_out, latent_code, batch_effect_removal_layer = Inference(\n",
        "        batch_data, latent_code_dim, T, encoder_layers, decoder_layers, exp_batch_id, re_use=re_use_flag)\n",
        "\n",
        "    _ = Cal_Loss(layer_out, batch_data, use_mask, batch_effect_removal_layer)\n",
        "\n",
        "    losses = tf.get_collection('losses', scope)\n",
        "\n",
        "    total_loss = tf.add_n(losses, name='total_loss')\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def Cal_Loss(outpout_layer_list, input_data, use_mask, removed_exp_batch_effect):\n",
        "    '''\n",
        "    Loss function of scScope.\n",
        "    Parameter:\n",
        "            outpout_layer_list:     encoder output of T recurrent structures in scScope.\n",
        "            input_data:             original gene expression matrix inputted into scScope.\n",
        "            use_mask:               flag indicating only use non-zero genes to calculate losses.\n",
        "            removed_exp_batch_effect:\n",
        "    Output:\n",
        "            acc_loss:               loss function value.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    input_data_corrected = input_data - removed_exp_batch_effect\n",
        "\n",
        "    if use_mask:\n",
        "        val_mask = tf.sign(input_data_corrected)\n",
        "    else:\n",
        "        val_mask = tf.sign(input_data_corrected + 1)\n",
        "\n",
        "    for i in range(len(outpout_layer_list)):\n",
        "        layer_out = outpout_layer_list[i]\n",
        "        if i == 0:\n",
        "            reconstruct_loss = tf.reduce_mean(\n",
        "                tf.norm(tf.multiply(val_mask, (layer_out - input_data_corrected))))\n",
        "        else:\n",
        "            reconstruct_loss = reconstruct_loss + \\\n",
        "                tf.reduce_mean(\n",
        "                    tf.norm(tf.multiply(val_mask, (layer_out - input_data_corrected))))\n",
        "    acc_loss = reconstruct_loss\n",
        "    tf.add_to_collection('losses', acc_loss)\n",
        "    return acc_loss\n",
        "\n",
        "\n",
        "def scalable_cluster(latent_code,\n",
        "                     kmeans_num=500,\n",
        "                     cluster_num=400,\n",
        "                     display_step=50,\n",
        "                     phenograh_neighbor=30\n",
        "                     ):\n",
        "    '''\n",
        "    Scalable  cluster:\n",
        "    To perform graph clustering on large-scale data, we designed a scalable clustering strategy by combining k-means and PhenoGraph.\n",
        "    Briefly, we divide cells into M (kmeans_num) groups of equal size and perform K-means (cluster_num) clustering on each group independently. \n",
        "\tThe whole dataset is split to M×K clusters and we only input the cluster centroids into PhenoGraph for graph clustering. \n",
        "\tFinally, each cell is assigned to graph clusters according to the cluster labels of its nearest centroids.\n",
        "    Parameters:\n",
        "        latent_code:    n*m matrix; n = number of cells, m = dimension of feature representation.\n",
        "        kmeans_num:     number of independent K-means clusterings used. This is also the subset number.\n",
        "        cluster_num:    cluster number for each K-means clustering. This is also the \"n_clusters\" in KMeans function in sklearn package.\n",
        "        display_step:   displaying the process of K-means clustering.\n",
        "        phenograh_neighbor: \"k\" parameter in PhenoGraph package.\n",
        "\t\t\n",
        "    Output:\n",
        "            label:          Cluster labels for input cells.\n",
        "    Altschuler & Wu Lab 2018.\n",
        "    Software provided as is under Apache License 2.0.\n",
        "    '''\n",
        "\n",
        "    print('Scalable clustering:')\n",
        "    print('Use %d subsets of cells for initially clustering...' % kmeans_num)\n",
        "\n",
        "    stamp = np.floor(np.linspace(0, latent_code.shape[0], kmeans_num + 1))\n",
        "    stamp = stamp.astype(int)\n",
        "\n",
        "    cluster_ceter = np.zeros([kmeans_num * cluster_num, latent_code.shape[1]])\n",
        "    mapping_sample_kmeans = np.zeros(latent_code.shape[0])\n",
        "\n",
        "    for i in range(kmeans_num):\n",
        "\n",
        "        low_bound = stamp[i]\n",
        "        upp_bound = stamp[i + 1]\n",
        "        sample_range = np.arange(low_bound, upp_bound)\n",
        "        select_sample = latent_code[sample_range, :]\n",
        "\n",
        "        kmeans = KMeans(n_clusters=cluster_num,\n",
        "                        random_state=0).fit(select_sample)\n",
        "        label = kmeans.labels_\n",
        "\n",
        "        for j in range(cluster_num):\n",
        "            cluster_sample_idx = np.nonzero(label == j)[0]\n",
        "            cluster_sample = select_sample[cluster_sample_idx, :]\n",
        "            cluster_ceter[i * cluster_num + j,\n",
        "                          :] = np.mean(cluster_sample, axis=0)\n",
        "            mapping_sample_kmeans[sample_range[cluster_sample_idx]\n",
        "                                  ] = i * cluster_num + j\n",
        "\n",
        "        if i % display_step == 0:\n",
        "            print('\\tK-means clustering for %d subset.' % i)\n",
        "\n",
        "    print('Finish intially clustering by K-means.')\n",
        "    print('Start PhenoGraph clustering...\\n')\n",
        "\n",
        "    label_pheno, graph, Q = phenograph.cluster(\n",
        "        cluster_ceter, k=phenograh_neighbor, n_jobs=1)\n",
        "\n",
        "    label = np.zeros(latent_code.shape[0])\n",
        "    for i in range(label_pheno.max() + 1):\n",
        "        center_index = np.nonzero(label_pheno == i)[0]\n",
        "        for j in center_index:\n",
        "            sample_index = np.nonzero(mapping_sample_kmeans == j)[\n",
        "                0]  # samples belong to this center\n",
        "            label[sample_index] = i\n",
        "    print('Finish density down-sampling clustering.')\n",
        "\n",
        "    return label"
      ],
      "execution_count": 0,
      "outputs": []
    },
